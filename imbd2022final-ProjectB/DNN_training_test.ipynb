{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "path1 = 'train1_data/'\n",
    "path2 = 'train2_data/'\n",
    "path3 = 'test_data/'\n",
    "train1_len = 46\n",
    "\n",
    "print('Loading train1 sg data...')\n",
    "sg_B1 = np.load(path1 + 'sg_B.npy')\n",
    "sg_C1 = np.load(path1 + 'sg_C.npy')\n",
    "sg_D1 = np.load(path1 + 'sg_D.npy')\n",
    "sg_E1 = np.load(path1 + 'sg_E.npy')\n",
    "sg_F1 = np.load(path1 + 'sg_F.npy')\n",
    "sg_G1 = np.load(path1 + 'sg_G.npy')\n",
    "sg_H1 = np.load(path1 + 'sg_H.npy')\n",
    "sg_I1 = np.load(path1 + 'sg_I.npy')\n",
    "BDF_distance1 = np.load(path1 + 'BDF_distance.npy')\n",
    "\n",
    "print('Loading train1 spike data...')\n",
    "spike_A1 = np.load(path1 + 'spike_A.npy')\n",
    "spike_B1 = np.load(path1 + 'spike_B.npy')\n",
    "spike_C1 = np.load(path1 + 'spike_C.npy')\n",
    "spike_D1 = np.load(path1 + 'spike_D.npy')\n",
    "spike_abs_B1 = np.load(path1 + 'spike_abs_B.npy')\n",
    "spike_abs_C1 = np.load(path1 + 'spike_abs_C.npy')\n",
    "spike_abs_D1 = np.load(path1 + 'spike_abs_D.npy')\n",
    "\n",
    "print('Loading train1 other data...')\n",
    "spike_B_lower_noise1 = np.load(path1 + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise1 = np.load(path1 + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise1 = np.load(path1 + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise1 = np.load(path1 + 'spike_abs_C_lower_noise.npy')\n",
    "\n",
    "BCD_distance1 = np.load(path1 + 'BCD_distance.npy')\n",
    "BCD_abs_distance1 = np.load(path1 + 'BCD_abs_distance.npy')\n",
    "\n",
    "print('Loading train2 sg data...')\n",
    "sg_B2 = np.load(path2 + 'sg_B.npy')\n",
    "sg_C2 = np.load(path2 + 'sg_C.npy')\n",
    "sg_D2 = np.load(path2 + 'sg_D.npy')\n",
    "sg_E2 = np.load(path2 + 'sg_E.npy')\n",
    "sg_F2 = np.load(path2 + 'sg_F.npy')\n",
    "sg_G2 = np.load(path2 + 'sg_G.npy')\n",
    "sg_H2 = np.load(path2 + 'sg_H.npy')\n",
    "sg_I2 = np.load(path2 + 'sg_I.npy')\n",
    "BDF_distance2 = np.load(path2 + 'BDF_distance.npy')\n",
    "\n",
    "print('Loading train2 spike data...')\n",
    "spike_A2 = np.load(path2 + 'spike_A.npy')\n",
    "spike_B2 = np.load(path2 + 'spike_B.npy')\n",
    "spike_C2 = np.load(path2 + 'spike_C.npy')\n",
    "spike_D2 = np.load(path2 + 'spike_D.npy')\n",
    "spike_abs_B2 = np.load(path2 + 'spike_abs_B.npy')\n",
    "spike_abs_C2 = np.load(path2 + 'spike_abs_C.npy')\n",
    "spike_abs_D2 = np.load(path2 + 'spike_abs_D.npy')\n",
    "\n",
    "print('Loading train2 other data...')\n",
    "spike_B_lower_noise2 = np.load(path2 + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise2 = np.load(path2 + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise2 = np.load(path2 + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise2 = np.load(path2 + 'spike_abs_C_lower_noise.npy')\n",
    "\n",
    "BCD_distance2 = np.load(path2 + 'BCD_distance.npy')\n",
    "BCD_abs_distance2 = np.load(path2 + 'BCD_abs_distance.npy')\n",
    "\n",
    "print('Concatenating train1 and train2 data...')\n",
    "# Concatenate the data\n",
    "sg_B = np.concatenate((sg_B1, sg_B2), axis=0)\n",
    "sg_C = np.concatenate((sg_C1, sg_C2), axis=0)\n",
    "sg_D = np.concatenate((sg_D1, sg_D2), axis=0)\n",
    "sg_E = np.concatenate((sg_E1, sg_E2), axis=0)\n",
    "sg_F = np.concatenate((sg_F1, sg_F2), axis=0)\n",
    "sg_G = np.concatenate((sg_G1, sg_G2), axis=0)\n",
    "sg_H = np.concatenate((sg_H1, sg_H2), axis=0)\n",
    "sg_I = np.concatenate((sg_I1, sg_I2), axis=0)\n",
    "BDF_distance = np.concatenate((BDF_distance1, BDF_distance2), axis=0)\n",
    "\n",
    "\n",
    "spike_A = np.concatenate((spike_A1, spike_A2), axis=0)\n",
    "spike_B = np.concatenate((spike_B1, spike_B2), axis=0)\n",
    "spike_C = np.concatenate((spike_C1, spike_C2), axis=0)\n",
    "spike_D = np.concatenate((spike_D1, spike_D2), axis=0)\n",
    "spike_abs_B = np.concatenate((spike_abs_B1, spike_abs_B2), axis=0)\n",
    "spike_abs_C = np.concatenate((spike_abs_C1, spike_abs_C2), axis=0)\n",
    "spike_abs_D = np.concatenate((spike_abs_D1, spike_abs_D2), axis=0)\n",
    "\n",
    "spike_B_lower_noise = np.concatenate((spike_B_lower_noise1, spike_B_lower_noise2), axis=0)\n",
    "spike_C_lower_noise = np.concatenate((spike_C_lower_noise1, spike_C_lower_noise2), axis=0)\n",
    "spike_abs_B_lower_noise = np.concatenate((spike_abs_B_lower_noise1, spike_abs_B_lower_noise2), axis=0)\n",
    "spike_abs_C_lower_noise = np.concatenate((spike_abs_C_lower_noise1, spike_abs_C_lower_noise2), axis=0)\n",
    "\n",
    "BCD_distance = np.concatenate((BCD_distance1, BCD_distance2), axis=0)\n",
    "BCD_abs_distance = np.concatenate((BCD_abs_distance1, BCD_abs_distance2), axis=0)\n",
    "\n",
    "output1 = pd.read_csv('train1/00_Wear_data.csv').loc[:, 'MaxWear']\n",
    "output2 = pd.read_csv('train2/00_Wear_data.csv').loc[:, 'MaxWear']\n",
    "Output = pd.concat([output1, output2], axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_B_sum = spike_B.sum(axis=1)\n",
    "spike_C_sum = spike_C.sum(axis=1)\n",
    "spike_D_sum = spike_D.sum(axis=1)\n",
    "\n",
    "spike_B_lower_noise_sum = spike_B_lower_noise.sum(axis=1)\n",
    "spike_C_lower_noise_sum = spike_C_lower_noise.sum(axis=1)\n",
    "\n",
    "spike_B_sg_B_sum = np.array([spike_B[i]-sg_B[i] for i in range(len(spike_B))]).sum(axis=1)\n",
    "spike_C_sg_D_sum = np.array([spike_C[i]-sg_D[i]\n",
    "                       for i in range(len(spike_C))]).sum(axis=1)\n",
    "spike_D_sg_F_sum = np.array([spike_D[i]-sg_F[i]\n",
    "                       for i in range(len(spike_D))]).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "integrated_spike_B1 = [sum(abs(spike_B_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_B2 = [sum(abs(spike_B_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_B_sum)+1)]\n",
    "integrated_spike_B = integrated_spike_B1 + integrated_spike_B2\n",
    "df2['integrated_spike_B'] = pd.Series(integrated_spike_B)\n",
    "\n",
    "\n",
    "integrated_spike_C1 = [sum(abs(spike_C_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_C2 = [sum(abs(spike_C_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_C_sum)+1)]\n",
    "integrated_spike_C = integrated_spike_C1 + integrated_spike_C2\n",
    "df2['integrated_spike_C'] = pd.Series(integrated_spike_C)\n",
    "\n",
    "integrated_spike_D1 = [sum(abs(spike_D_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_D2 = [sum(abs(spike_D_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_D_sum)+1)]\n",
    "integrated_spike_D = integrated_spike_D1 + integrated_spike_D2\n",
    "df2['integrated_spike_D'] = pd.Series(integrated_spike_D)\n",
    "\n",
    "integrated_spike_B_lower_noise1 = [sum(abs(\n",
    "    spike_B_lower_noise_sum[:i]))for i in range(1, train1_len+1)]\n",
    "integrated_spike_B_lower_noise2 = [sum(abs(\n",
    "    spike_B_lower_noise_sum[train1_len+1:i]))for i in range(train1_len+1, len(spike_B_lower_noise_sum)+1)]\n",
    "integrated_spike_B_lower_noise = integrated_spike_B_lower_noise1 + \\\n",
    "    integrated_spike_B_lower_noise2\n",
    "df2['integrated_spike_B_lower_noise'] = pd.Series(\n",
    "    integrated_spike_B_lower_noise)\n",
    "\n",
    "integrated_spike_C_lower_noise1 = [sum(abs(\n",
    "    spike_C_lower_noise_sum[:i]))for i in range(1, train1_len+1)]\n",
    "integrated_spike_C_lower_noise2 = [sum(abs(\n",
    "    spike_C_lower_noise_sum[i-1:i]))for i in range(train1_len+1, len(spike_C_lower_noise_sum)+1)]\n",
    "integrated_spike_C_lower_noise = integrated_spike_C_lower_noise1 + \\\n",
    "    integrated_spike_C_lower_noise2\n",
    "df2['integrated_spike_C_lower_noise'] = pd.Series(\n",
    "    integrated_spike_C_lower_noise)\n",
    "\n",
    "integrated_spike_B_sg_B1 = [sum(abs(spike_B_sg_B_sum[:i]))\n",
    "                            for i in range(1, train1_len+1)]\n",
    "integrated_spike_B_sg_B2 = [sum(abs(spike_B_sg_B_sum[train1_len+1:i]))\n",
    "                            for i in range(train1_len+1, len(spike_B_sg_B_sum)+1)]\n",
    "integrated_spike_B_sg_B = integrated_spike_B_sg_B1 + integrated_spike_B_sg_B2\n",
    "df2['integrated_spike_B_sg_B'] = pd.Series(integrated_spike_B_sg_B)\n",
    "\n",
    "integrated_spike_C_sg_D1 = [sum(abs(spike_C_sg_D_sum[:i]))\n",
    "                            for i in range(1, train1_len+1)]\n",
    "integrated_spike_C_sg_D2 = [sum(abs(spike_C_sg_D_sum[train1_len+1:i]))\n",
    "                            for i in range(train1_len+1, len(spike_C_sg_D_sum)+1)]\n",
    "integrated_spike_C_sg_D = integrated_spike_C_sg_D1 + integrated_spike_C_sg_D2\n",
    "df2['integrated_spike_C_sg_D'] = pd.Series(integrated_spike_C_sg_D)\n",
    "\n",
    "integrated_spike_D_sg_F1 = [sum(abs(spike_D_sg_F_sum[:i]))\n",
    "                            for i in range(1, train1_len+1)]\n",
    "integrated_spike_D_sg_F2 = [sum(abs(spike_D_sg_F_sum[train1_len+1:i]))\n",
    "                            for i in range(train1_len+1, len(spike_D_sg_F_sum)+1)]\n",
    "integrated_spike_D_sg_F = integrated_spike_D_sg_F1 + integrated_spike_D_sg_F2\n",
    "df2['integrated_spike_D_sg_F'] = pd.Series(integrated_spike_D_sg_F)\n",
    "\n",
    "sg_C_sum = sg_C.sum(axis=1)\n",
    "integrated_sg_C_sum1 = [sum(abs(sg_C_sum[:i]))\n",
    "                        for i in range(1, train1_len+1)]\n",
    "integrated_sg_C_sum2 = [sum(abs(sg_C_sum[train1_len+1:i]))\n",
    "                        for i in range(train1_len+1, len(sg_C_sum)+1)]\n",
    "integrated_sg_C_sum = integrated_sg_C_sum1 + integrated_sg_C_sum2\n",
    "df2['integrated_sg_C_sum'] = pd.Series(integrated_sg_C_sum)\n",
    "\n",
    "df2['Output'] = pd.Series(Output)\n",
    "\n",
    "columns = df2.columns\n",
    "transformer = StandardScaler()\n",
    "corr = pd.DataFrame(transformer.fit_transform(df2), columns=columns).corr()\n",
    "print(corr.iloc[-1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.columns)\n",
    "Model = df2.values\n",
    "# Model = shuffle(Model, random_state=42)\n",
    "Input = Model[:, :-1]\n",
    "output = Model[:, -1]\n",
    "output = np.reshape(output, (-1, 1))\n",
    "\n",
    "Input_shape = Input.shape[1]\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n",
    "print('Output layer ~ 10: ', output[:10])\n",
    "\n",
    "Input_transformer = MaxAbsScaler()\n",
    "Output_transformer = StandardScaler()\n",
    "\n",
    "Input = Input_transformer.fit_transform(Input)\n",
    "Output = Output_transformer.fit_transform(output)\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, output_train, output_test = train_test_split(\n",
    "  Input, output, test_size=0.1, random_state=42)\n",
    "# input_train, input_test, output_train, output_test = Input, Input, output, output\n",
    "print('input_train.shape:\\t', input_train.shape)\n",
    "print('input_test.shape:\\t', input_test.shape)\n",
    "print('output_train.shape:\\t', output_train.shape)\n",
    "print('output_test.shape:\\t', output_test.shape)\n",
    "\n",
    "input_train = tf.convert_to_tensor(input_train)\n",
    "input_test = tf.convert_to_tensor(input_test)\n",
    "output_train = tf.convert_to_tensor(output_train)\n",
    "output_test = tf.convert_to_tensor(output_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=Input_shape))\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "#model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 15\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "# Adam RMSprop\n",
    "model.compile(optimizer=tf.optimizers.Adam(\n",
    "    learning_rate=learning_rate), loss=rmse)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(input_train, output_train,\n",
    "                    validation_data=(input_test, output_test), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Training cost time:\\t', end - start, 's')\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_rmse(actual, predict):\n",
    "    return pow(np.mean(pow(actual - predict, 2)), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(input_test)\n",
    "test_predict_actual = output_test.numpy()\n",
    "\n",
    "test_predict = np.array(test_predict)\n",
    "\n",
    "test_predict = Output_transformer.inverse_transform(test_predict)\n",
    "test_predict_actual = Output_transformer.inverse_transform(test_predict_actual)\n",
    "\n",
    "test_RMSE = numpy_rmse(test_predict_actual, test_predict)\n",
    "print('Test: ')\n",
    "print('Test RMSE:\\t', test_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = model.predict(input_train)\n",
    "train_predict_actual = output_train.numpy()\n",
    "\n",
    "train_predict = Output_transformer.inverse_transform(train_predict)\n",
    "train_predict_actual = Output_transformer.inverse_transform(\n",
    "    train_predict_actual)\n",
    "\n",
    "train_RMSE = numpy_rmse(train_predict_actual, train_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Train: ')\n",
    "print('Train RMSE:\\t', train_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predict = model.predict(Input)\n",
    "total_predict_actual = output\n",
    "\n",
    "total_predict = Output_transformer.inverse_transform(total_predict)\n",
    "total_predict_actual = Output_transformer.inverse_transform(\n",
    "    total_predict_actual)\n",
    "\n",
    "total_RMSE = numpy_rmse(total_predict_actual, total_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Total: ')\n",
    "print('Total RMSE:\\t', total_RMSE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df60fffceb1f00e3f7f321aa5eaa19ef554ab7d6ad6d9939fc6be363c8659db3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
