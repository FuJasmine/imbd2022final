{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "path1 = 'train1_data/'\n",
    "path2 = 'train2_data/'\n",
    "path3 = 'test_data/'\n",
    "train1_len = 46\n",
    "\n",
    "print('Loading train1 sg data...')\n",
    "sg_B1 = np.load(path1 + 'sg_B.npy')\n",
    "sg_C1 = np.load(path1 + 'sg_C.npy')\n",
    "sg_D1 = np.load(path1 + 'sg_D.npy')\n",
    "sg_E1 = np.load(path1 + 'sg_E.npy')\n",
    "sg_F1 = np.load(path1 + 'sg_F.npy')\n",
    "sg_G1 = np.load(path1 + 'sg_G.npy')\n",
    "sg_H1 = np.load(path1 + 'sg_H.npy')\n",
    "sg_I1 = np.load(path1 + 'sg_I.npy')\n",
    "BDF_distance1 = np.load(path1 + 'BDF_distance.npy')\n",
    "\n",
    "print('Loading train1 spike data...')\n",
    "spike_A1 = np.load(path1 + 'spike_A.npy')\n",
    "spike_B1 = np.load(path1 + 'spike_B.npy')\n",
    "spike_C1 = np.load(path1 + 'spike_C.npy')\n",
    "spike_D1 = np.load(path1 + 'spike_D.npy')\n",
    "spike_abs_B1 = np.load(path1 + 'spike_abs_B.npy')\n",
    "spike_abs_C1 = np.load(path1 + 'spike_abs_C.npy')\n",
    "spike_abs_D1 = np.load(path1 + 'spike_abs_D.npy')\n",
    "\n",
    "print('Loading train1 other data...')\n",
    "spike_B_lower_noise1 = np.load(path1 + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise1 = np.load(path1 + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise1 = np.load(path1 + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise1 = np.load(path1 + 'spike_abs_C_lower_noise.npy')\n",
    "sg_B_lower_noise1 = np.load(path1 + 'sg_B_lower_noise.npy')\n",
    "sg_D_lower_noise1 = np.load(path1 + 'sg_D_lower_noise.npy')\n",
    "\n",
    "BCD_distance1 = np.load(path1 + 'BCD_distance.npy')\n",
    "BCD_abs_distance1 = np.load(path1 + 'BCD_abs_distance.npy')\n",
    "\n",
    "print('Loading train2 sg data...')\n",
    "sg_B2 = np.load(path2 + 'sg_B.npy')\n",
    "sg_C2 = np.load(path2 + 'sg_C.npy')\n",
    "sg_D2 = np.load(path2 + 'sg_D.npy')\n",
    "sg_E2 = np.load(path2 + 'sg_E.npy')\n",
    "sg_F2 = np.load(path2 + 'sg_F.npy')\n",
    "sg_G2 = np.load(path2 + 'sg_G.npy')\n",
    "sg_H2 = np.load(path2 + 'sg_H.npy')\n",
    "sg_I2 = np.load(path2 + 'sg_I.npy')\n",
    "BDF_distance2 = np.load(path2 + 'BDF_distance.npy')\n",
    "\n",
    "print('Loading train2 spike data...')\n",
    "spike_A2 = np.load(path2 + 'spike_A.npy')\n",
    "spike_B2 = np.load(path2 + 'spike_B.npy')\n",
    "spike_C2 = np.load(path2 + 'spike_C.npy')\n",
    "spike_D2 = np.load(path2 + 'spike_D.npy')\n",
    "spike_abs_B2 = np.load(path2 + 'spike_abs_B.npy')\n",
    "spike_abs_C2 = np.load(path2 + 'spike_abs_C.npy')\n",
    "spike_abs_D2 = np.load(path2 + 'spike_abs_D.npy')\n",
    "\n",
    "print('Loading train2 other data...')\n",
    "spike_B_lower_noise2 = np.load(path2 + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise2 = np.load(path2 + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise2 = np.load(path2 + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise2 = np.load(path2 + 'spike_abs_C_lower_noise.npy')\n",
    "sg_B_lower_noise2 = np.load(path2 + 'sg_B_lower_noise.npy')\n",
    "sg_D_lower_noise2 = np.load(path2 + 'sg_D_lower_noise.npy')\n",
    "\n",
    "BCD_distance2 = np.load(path2 + 'BCD_distance.npy')\n",
    "BCD_abs_distance2 = np.load(path2 + 'BCD_abs_distance.npy')\n",
    "\n",
    "print('Concatenating train1 and train2 data...')\n",
    "# Concatenate the data\n",
    "sg_B = np.concatenate((sg_B1, sg_B2), axis=0)\n",
    "sg_C = np.concatenate((sg_C1, sg_C2), axis=0)\n",
    "sg_D = np.concatenate((sg_D1, sg_D2), axis=0)\n",
    "sg_E = np.concatenate((sg_E1, sg_E2), axis=0)\n",
    "sg_F = np.concatenate((sg_F1, sg_F2), axis=0)\n",
    "sg_G = np.concatenate((sg_G1, sg_G2), axis=0)\n",
    "sg_H = np.concatenate((sg_H1, sg_H2), axis=0)\n",
    "sg_I = np.concatenate((sg_I1, sg_I2), axis=0)\n",
    "BDF_distance = np.concatenate((BDF_distance1, BDF_distance2), axis=0)\n",
    "\n",
    "\n",
    "spike_A = np.concatenate((spike_A1, spike_A2), axis=0)\n",
    "spike_B = np.concatenate((spike_B1, spike_B2), axis=0)\n",
    "spike_C = np.concatenate((spike_C1, spike_C2), axis=0)\n",
    "spike_D = np.concatenate((spike_D1, spike_D2), axis=0)\n",
    "spike_abs_B = np.concatenate((spike_abs_B1, spike_abs_B2), axis=0)\n",
    "spike_abs_C = np.concatenate((spike_abs_C1, spike_abs_C2), axis=0)\n",
    "spike_abs_D = np.concatenate((spike_abs_D1, spike_abs_D2), axis=0)\n",
    "\n",
    "spike_B_lower_noise = np.concatenate((spike_B_lower_noise1, spike_B_lower_noise2), axis=0)\n",
    "spike_C_lower_noise = np.concatenate((spike_C_lower_noise1, spike_C_lower_noise2), axis=0)\n",
    "spike_abs_B_lower_noise = np.concatenate((spike_abs_B_lower_noise1, spike_abs_B_lower_noise2), axis=0)\n",
    "spike_abs_C_lower_noise = np.concatenate((spike_abs_C_lower_noise1, spike_abs_C_lower_noise2), axis=0)\n",
    "sg_B_lower_noise = np.concatenate((sg_B_lower_noise1, sg_B_lower_noise2), axis=0)\n",
    "sg_D_lower_noise = np.concatenate((sg_D_lower_noise1, sg_D_lower_noise2), axis=0)\n",
    "\n",
    "BCD_distance = np.concatenate((BCD_distance1, BCD_distance2), axis=0)\n",
    "BCD_abs_distance = np.concatenate((BCD_abs_distance1, BCD_abs_distance2), axis=0)\n",
    "\n",
    "print('Loading Wear data...')\n",
    "output1 = pd.read_csv('train1/00_Wear_data.csv').loc[:, 'MaxWear']\n",
    "output2 = pd.read_csv('train2/00_Wear_data.csv').loc[:, 'MaxWear']\n",
    "Output = pd.concat([output1, output2], axis=0).values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spike_B_sum = spike_B.sum(axis=1)\n",
    "spike_C_sum = spike_C.sum(axis=1)\n",
    "spike_D_sum = spike_D.sum(axis=1)\n",
    "\n",
    "spike_B_lower_noise_sum = spike_B_lower_noise.sum(axis=1)\n",
    "spike_C_lower_noise_sum = spike_C_lower_noise.sum(axis=1)\n",
    "\n",
    "sg_C_sum = sg_C.sum(axis=1)\n",
    "\n",
    "sg_B_sum = sg_B.sum(axis=1)\n",
    "sg_D_sum = sg_D.sum(axis=1)\n",
    "sg_F_sum = sg_F.sum(axis=1)\n",
    "\n",
    "# Old featrues ↓\n",
    "# spike_B_sg_B_sum = np.array([spike_B[i]-sg_B[i] for i in range(len(spike_B))]).sum(axis=1)\n",
    "# spike_C_sg_D_sum = np.array([spike_C[i]-sg_D[i]\n",
    "#                        for i in range(len(spike_C))]).sum(axis=1)\n",
    "# spike_D_sg_F_sum = np.array([spike_D[i]-sg_F[i]\n",
    "#                              for i in range(len(spike_D))]).sum(axis=1)\n",
    "\n",
    "############################################# New features #############################################\n",
    "spike_B_sg_B_sum = np.array([spike_B[i]-abs(sg_B_lower_noise[i])\n",
    "                            for i in range(len(spike_B))]).sum(axis=1)\n",
    "spike_C_sg_D_sum = np.array([spike_C[i]-abs(sg_D_lower_noise[i])\n",
    "                             for i in range(len(spike_C))]).sum(axis=1)\n",
    "\n",
    "spike_B_sg_B_multi_sum = np.array([np.multiply(spike_B[i], sg_B[i]) for i in range(len(spike_B))]).sum(axis=1)\n",
    "spike_C_sg_D_multi_sum = np.array([np.multiply(spike_C[i], sg_D[i]) for i in range(len(spike_C))]).sum(axis=1)\n",
    "spike_D_sg_F_multi_sum = np.array([np.multiply(spike_D[i], sg_F[i]) for i in range(len(spike_D))]).sum(axis=1)\n",
    "############################################# New features #############################################\n",
    "\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#Spike_sum\n",
    "integrated_spike_B1 = [sum(abs(spike_B_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_B2 = [sum(abs(spike_B_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_B_sum)+1)]\n",
    "integrated_spike_B = integrated_spike_B1 + integrated_spike_B2\n",
    "df['integrated_spike_B'] = pd.Series(integrated_spike_B)\n",
    "\n",
    "\n",
    "integrated_spike_C1 = [sum(abs(spike_C_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_C2 = [sum(abs(spike_C_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_C_sum)+1)]\n",
    "integrated_spike_C = integrated_spike_C1 + integrated_spike_C2\n",
    "df['integrated_spike_C'] = pd.Series(integrated_spike_C)\n",
    "\n",
    "integrated_spike_D1 = [sum(abs(spike_D_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_D2 = [sum(abs(spike_D_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_D_sum)+1)]\n",
    "integrated_spike_D = integrated_spike_D1 + integrated_spike_D2\n",
    "df['integrated_spike_D'] = pd.Series(integrated_spike_D)\n",
    "\n",
    "#Lower noise\n",
    "integrated_spike_B_lower_noise1 = [sum(abs(\n",
    "    spike_B_lower_noise_sum[:i]))for i in range(1, train1_len+1)]\n",
    "integrated_spike_B_lower_noise2 = [sum(abs(\n",
    "    spike_B_lower_noise_sum[train1_len+1:i]))for i in range(train1_len+1, len(spike_B_lower_noise_sum)+1)]\n",
    "integrated_spike_B_lower_noise = integrated_spike_B_lower_noise1 + \\\n",
    "    integrated_spike_B_lower_noise2\n",
    "df['integrated_spike_B_lower_noise'] = pd.Series(\n",
    "    integrated_spike_B_lower_noise)\n",
    "\n",
    "integrated_spike_C_lower_noise1 = [sum(abs(\n",
    "    spike_C_lower_noise_sum[:i]))for i in range(1, train1_len+1)]\n",
    "integrated_spike_C_lower_noise2 = [sum(abs(\n",
    "    spike_C_lower_noise_sum[i-1:i]))for i in range(train1_len+1, len(spike_C_lower_noise_sum)+1)]\n",
    "integrated_spike_C_lower_noise = integrated_spike_C_lower_noise1 + \\\n",
    "    integrated_spike_C_lower_noise2\n",
    "df['integrated_spike_C_lower_noise'] = pd.Series(\n",
    "    integrated_spike_C_lower_noise)\n",
    "\n",
    "#Spike - abs(sg_lower_noise)\n",
    "integrated_spike_B_sg_B1 = [sum(abs(spike_B_sg_B_sum[:i]))\n",
    "                            for i in range(1, train1_len+1)]\n",
    "integrated_spike_B_sg_B2 = [sum(abs(spike_B_sg_B_sum[train1_len+1:i]))\n",
    "                            for i in range(train1_len+1, len(spike_B_sg_B_sum)+1)]\n",
    "integrated_spike_B_sg_B = integrated_spike_B_sg_B1 + integrated_spike_B_sg_B2\n",
    "df['integrated_spike_B_sg_B'] = pd.Series(integrated_spike_B_sg_B)\n",
    "\n",
    "integrated_spike_C_sg_D1 = [sum(abs(spike_C_sg_D_sum[:i]))\n",
    "                            for i in range(1, train1_len+1)]\n",
    "integrated_spike_C_sg_D2 = [sum(abs(spike_C_sg_D_sum[train1_len+1:i]))\n",
    "                            for i in range(train1_len+1, len(spike_C_sg_D_sum)+1)]\n",
    "integrated_spike_C_sg_D = integrated_spike_C_sg_D1 + integrated_spike_C_sg_D2\n",
    "df['integrated_spike_C_sg_D'] = pd.Series(integrated_spike_C_sg_D)\n",
    "\n",
    "integrated_sg_C_sum1 = [sum(abs(sg_C_sum[:i]))\n",
    "                        for i in range(1, train1_len+1)]\n",
    "integrated_sg_C_sum2 = [sum(abs(sg_C_sum[train1_len+1:i]))\n",
    "                        for i in range(train1_len+1, len(sg_C_sum)+1)]\n",
    "integrated_sg_C_sum = integrated_sg_C_sum1 + integrated_sg_C_sum2\n",
    "df['integrated_sg_C_sum'] = pd.Series(integrated_sg_C_sum)\n",
    "\n",
    "\n",
    "############################################# New features #############################################\n",
    "#Sg_sum\n",
    "\n",
    "integrated_sg_B_sum1 = [sum(abs(sg_B_sum[:i]))\n",
    "                        for i in range(1, train1_len+1)]\n",
    "integrated_sg_B_sum2 = [sum(abs(sg_B_sum[train1_len+1:i]))\n",
    "                        for i in range(train1_len+1, len(sg_B_sum)+1)]\n",
    "integrated_sg_B_sum = integrated_sg_B_sum1 + integrated_sg_B_sum2\n",
    "df['integrated_sg_B_sum'] = pd.Series(integrated_sg_B_sum)\n",
    "\n",
    "integrated_sg_D_sum1 = [sum(abs(sg_D_sum[:i]))\n",
    "                        for i in range(1, train1_len+1)]\n",
    "integrated_sg_D_sum2 = [sum(abs(sg_D_sum[train1_len+1:i]))\n",
    "                        for i in range(train1_len+1, len(sg_D_sum)+1)]\n",
    "integrated_sg_D_sum = integrated_sg_D_sum1 + integrated_sg_D_sum2\n",
    "df['integrated_sg_D_sum'] = pd.Series(integrated_sg_D_sum)\n",
    "\n",
    "integrated_sg_F_sum1 = [sum(abs(sg_F_sum[:i]))\n",
    "                        for i in range(1, train1_len+1)]\n",
    "integrated_sg_F_sum2 = [sum(abs(sg_F_sum[train1_len+1:i]))\n",
    "                        for i in range(train1_len+1, len(sg_F_sum)+1)]\n",
    "integrated_sg_F_sum = integrated_sg_F_sum1 + integrated_sg_F_sum2\n",
    "df['integrated_sg_F_sum'] = pd.Series(integrated_sg_F_sum)\n",
    "\n",
    "# sg*spike\n",
    "integrated_spike_B_sg_B_multi1 = [sum(abs(spike_B_sg_B_multi_sum[:i]))\n",
    "                                  for i in range(1, train1_len+1)]\n",
    "integrated_spike_B_sg_B_multi2 = [sum(abs(spike_B_sg_B_multi_sum[train1_len+1:i]))\n",
    "                                  for i in range(train1_len+1, len(spike_B_sg_B_multi_sum)+1)]\n",
    "integrated_spike_B_sg_B_multi = integrated_spike_B_sg_B_multi1 + \\\n",
    "    integrated_spike_B_sg_B_multi2\n",
    "df['integrated_spike_B_sg_B_multi'] = pd.Series(integrated_spike_B_sg_B_multi)\n",
    "\n",
    "integrated_spike_C_sg_D_multi1 = [sum(abs(spike_C_sg_D_multi_sum[:i]))\n",
    "                                  for i in range(1, train1_len+1)]\n",
    "integrated_spike_C_sg_D_multi2 = [sum(abs(spike_C_sg_D_multi_sum[train1_len+1:i]))\n",
    "                                  for i in range(train1_len+1, len(spike_C_sg_D_multi_sum)+1)]\n",
    "integrated_spike_C_sg_D_multi = integrated_spike_C_sg_D_multi1 + \\\n",
    "    integrated_spike_C_sg_D_multi2\n",
    "df['integrated_spike_C_sg_D_multi'] = pd.Series(integrated_spike_C_sg_D_multi)\n",
    "\n",
    "integrated_spike_D_sg_F_multi1 = [sum(abs(spike_D_sg_F_multi_sum[:i]))\n",
    "                                  for i in range(1, train1_len+1)]\n",
    "integrated_spike_D_sg_F_multi2 = [sum(abs(spike_D_sg_F_multi_sum[train1_len+1:i]))\n",
    "                                  for i in range(train1_len+1, len(spike_D_sg_F_multi_sum)+1)]\n",
    "integrated_spike_D_sg_F_multi = integrated_spike_D_sg_F_multi1 + \\\n",
    "    integrated_spike_D_sg_F_multi2\n",
    "df['integrated_spike_D_sg_F_multi'] = pd.Series(integrated_spike_D_sg_F_multi)\n",
    "############################################# New features #############################################\n",
    "\n",
    "df['Output'] = pd.Series(Output)\n",
    "\n",
    "columns = df.columns\n",
    "transformer = StandardScaler()\n",
    "corr = pd.DataFrame(transformer.fit_transform(df), columns=columns).corr()\n",
    "print(corr.iloc[-1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)\n",
    "Model = df.values\n",
    "Model = shuffle(Model, random_state=0)\n",
    "Input = Model[:, :-1]\n",
    "Output = Model[:, -1]\n",
    "Output = np.reshape(Output, (-1, 1))\n",
    "\n",
    "Input_shape = Input.shape[1]\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n",
    "print('Output layer ~ 10: ', Output[:10])\n",
    "\n",
    "Input_transformer = StandardScaler()     # MaxAbsScaler\n",
    "Output_transformer = StandardScaler()\n",
    "\n",
    "Input = Input_transformer.fit_transform(Input)\n",
    "#Output = Output_transformer.fit_transform(Output)\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_train, input_test, output_train, output_test = train_test_split(\n",
    "#    Input, Output, test_size=0.1, random_state=0)\n",
    "input_train, input_test, output_train, output_test = Input, Input, Output, Output\n",
    "print('input_train.shape:\\t', input_train.shape)\n",
    "print('input_test.shape:\\t', input_test.shape)\n",
    "print('output_train.shape:\\t', output_train.shape)\n",
    "print('output_test.shape:\\t', output_test.shape)\n",
    "\n",
    "input_train = tf.convert_to_tensor(input_train)\n",
    "input_test = tf.convert_to_tensor(input_test)\n",
    "output_train = tf.convert_to_tensor(output_train)\n",
    "output_test = tf.convert_to_tensor(output_test)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=Input_shape))\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "#model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "learning_rate = 0.003\n",
    "batch_size = 20\n",
    "epochs = 500    # 1000\n",
    "\n",
    "\n",
    "# Adam RMSprop\n",
    "start = time.time()\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate), loss=rmse)\n",
    "history = model.fit(input_train, output_train,\n",
    "                    validation_data=(input_test, output_test), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(learning_rate=learning_rate/10), loss=rmse)\n",
    "history = model.fit(input_train, output_train,\n",
    "                    validation_data=(input_test, output_test), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Training cost time:\\t', end - start, 's')\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_rmse(actual, predict):\n",
    "    return pow(np.mean(pow(actual - predict, 2)), 0.5)\n",
    "\n",
    "test_predict = model.predict(input_test)\n",
    "test_predict_actual = output_test.numpy()\n",
    "\n",
    "test_predict = np.array(test_predict)\n",
    "\n",
    "print()\n",
    "print(test_predict.shape)\n",
    "print()\n",
    "\n",
    "#test_predict = Output_transformer.inverse_transform(test_predict)\n",
    "#test_predict_actual = Output_transformer.inverse_transform(test_predict_actual)\n",
    "\n",
    "test_RMSE = numpy_rmse(test_predict_actual, test_predict)\n",
    "print('Test: ')\n",
    "print('Test RMSE:\\t', test_RMSE)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "train_predict = model.predict(input_train)\n",
    "train_predict_actual = output_train.numpy()\n",
    "\n",
    "#train_predict = Output_transformer.inverse_transform(train_predict)\n",
    "#train_predict_actual = Output_transformer.inverse_transform(train_predict_actual)\n",
    "\n",
    "train_RMSE = numpy_rmse(train_predict_actual, train_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Train: ')\n",
    "print('Train RMSE:\\t', train_RMSE)\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "total_predict = model.predict(Input)\n",
    "total_predict_actual = Output\n",
    "\n",
    "#total_predict = Output_transformer.inverse_transform(total_predict)\n",
    "#total_predict_actual = Output_transformer.inverse_transform(total_predict_actual)\n",
    "\n",
    "total_RMSE = numpy_rmse(total_predict_actual, total_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Total: ')\n",
    "print('Total RMSE:\\t', total_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading test sg data...')\n",
    "sg_B = np.load(path3 + 'sg_B.npy')\n",
    "sg_C = np.load(path3 + 'sg_C.npy')\n",
    "sg_D = np.load(path3 + 'sg_D.npy')\n",
    "sg_E = np.load(path3 + 'sg_E.npy')\n",
    "sg_F = np.load(path3 + 'sg_F.npy')\n",
    "sg_G = np.load(path3 + 'sg_G.npy')\n",
    "sg_H = np.load(path3 + 'sg_H.npy')\n",
    "sg_I = np.load(path3 + 'sg_I.npy')\n",
    "BDF_distance = np.load(path3 + 'BDF_distance.npy')\n",
    "\n",
    "print('Loading test spike data...')\n",
    "spike_A = np.load(path3 + 'spike_A.npy')\n",
    "spike_B = np.load(path3 + 'spike_B.npy')\n",
    "spike_C = np.load(path3 + 'spike_C.npy')\n",
    "spike_D = np.load(path3 + 'spike_D.npy')\n",
    "spike_abs_B = np.load(path3 + 'spike_abs_B.npy')\n",
    "spike_abs_C = np.load(path3 + 'spike_abs_C.npy')\n",
    "spike_abs_D = np.load(path3 + 'spike_abs_D.npy')\n",
    "\n",
    "print('Loading test other data...')\n",
    "spike_B_lower_noise = np.load(path3 + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise = np.load(path3 + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise = np.load(path3 + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise = np.load(path3 + 'spike_abs_C_lower_noise.npy')\n",
    "sg_B_lower_noise = np.load(path3 + 'sg_B_lower_noise.npy')\n",
    "sg_D_lower_noise = np.load(path3 + 'sg_D_lower_noise.npy')\n",
    "\n",
    "BCD_distance = np.load(path3 + 'BCD_distance.npy')\n",
    "BCD_abs_distance = np.load(path3 + 'BCD_abs_distance.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('preprocessing test data...')\n",
    "spike_B_sum = spike_B.sum(axis=1)\n",
    "spike_C_sum = spike_C.sum(axis=1)\n",
    "spike_D_sum = spike_D.sum(axis=1)\n",
    "\n",
    "spike_B_lower_noise_sum = spike_B_lower_noise.sum(axis=1)\n",
    "spike_C_lower_noise_sum = spike_C_lower_noise.sum(axis=1)\n",
    "\n",
    "# Old featrues ↓\n",
    "# spike_B_sg_B_sum = np.array([spike_B[i]-sg_B[i] for i in range(len(spike_B))]).sum(axis=1)\n",
    "# spike_C_sg_D_sum = np.array([spike_C[i]-sg_D[i]\n",
    "#                        for i in range(len(spike_C))]).sum(axis=1)\n",
    "# spike_D_sg_F_sum = np.array([spike_D[i]-sg_F[i]\n",
    "#                              for i in range(len(spike_D))]).sum(axis=1)\n",
    "\n",
    "############################################# New features #############################################\n",
    "spike_B_sg_B_sum = np.array([spike_B[i]-abs(sg_B_lower_noise[i])\n",
    "                            for i in range(len(spike_B))]).sum(axis=1)\n",
    "spike_C_sg_D_sum = np.array([spike_C[i]-abs(sg_D_lower_noise[i])\n",
    "                             for i in range(len(spike_C))]).sum(axis=1)\n",
    "\n",
    "spike_B_sg_B_multi_sum = np.array(\n",
    "    [np.multiply(spike_B[i], sg_B[i]) for i in range(len(spike_B))]).sum(axis=1)\n",
    "spike_C_sg_D_multi_sum = np.array(\n",
    "    [np.multiply(spike_C[i], sg_D[i]) for i in range(len(spike_C))]).sum(axis=1)\n",
    "spike_D_sg_F_multi_sum = np.array(\n",
    "    [np.multiply(spike_D[i], sg_F[i]) for i in range(len(spike_D))]).sum(axis=1)\n",
    "############################################# New features #############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame()\n",
    "integrated_spike_B = [sum(abs(spike_B_sum[:i]))\n",
    "                      for i in range(1, len(spike_B_sum)+1)]\n",
    "df2['integrated_spike_B'] = pd.Series(integrated_spike_B)\n",
    "\n",
    "integrated_spike_C = [sum(abs(spike_C_sum[:i]))\n",
    "                      for i in range(1, len(spike_C_sum)+1)]\n",
    "df2['integrated_spike_C'] = pd.Series(integrated_spike_C)\n",
    "\n",
    "integrated_spike_D = [sum(abs(spike_D_sum[:i]))\n",
    "                      for i in range(1, len(spike_D_sum)+1)]\n",
    "df2['integrated_spike_D'] = pd.Series(integrated_spike_D)\n",
    "\n",
    "integrated_spike_B_lower_noise = [sum(abs(\n",
    "    spike_B_lower_noise_sum[:i]))for i in range(1, len(spike_B_lower_noise_sum)+1)]\n",
    "df2['integrated_spike_B_lower_noise'] = pd.Series(\n",
    "    integrated_spike_B_lower_noise)\n",
    "\n",
    "integrated_spike_C_lower_noise = [sum(abs(\n",
    "    spike_C_lower_noise_sum[:i]))for i in range(1, len(spike_C_lower_noise_sum)+1)]\n",
    "df2['integrated_spike_C_lower_noise'] = pd.Series(\n",
    "    integrated_spike_C_lower_noise)\n",
    "\n",
    "integrated_spike_B_sg_B_sum = [sum(abs(spike_B_sg_B_sum[:i]))\n",
    "                      for i in range(1, len(spike_B_sg_B_sum)+1)]\n",
    "df2['integrated_spike_B_sg_B_sum'] = pd.Series(integrated_spike_B_sg_B_sum)\n",
    "\n",
    "integrated_spike_C_sg_D_sum = [sum(abs(spike_C_sg_D_sum[:i]))\n",
    "                               for i in range(1, len(spike_C_sg_D_sum)+1)]\n",
    "df2['integrated_spike_C_sg_D_sum'] = pd.Series(integrated_spike_C_sg_D_sum)\n",
    "\n",
    "integrated_sg_C_sum = [sum(abs(sg_C_sum[:i]))\n",
    "                       for i in range(1, len(sg_C_sum)+1)]\n",
    "df2['integrated_sg_C_sum'] = pd.Series(integrated_sg_C_sum)\n",
    "\n",
    "############################################# New features #############################################\n",
    "#Sg_sum\n",
    "integrated_sg_B_sum = [sum(abs(sg_B_sum[:i]))\n",
    "                        for i in range(1, len(sg_B_sum)+1)]\n",
    "df2['integrated_sg_B_sum'] = pd.Series(integrated_sg_B_sum)\n",
    "\n",
    "integrated_sg_D_sum = [sum(abs(sg_D_sum[:i]))\n",
    "                       for i in range(1, len(sg_D_sum)+1)]\n",
    "df2['integrated_sg_D_sum'] = pd.Series(integrated_sg_D_sum)\n",
    "\n",
    "integrated_sg_F_sum = [sum(abs(sg_F_sum[:i]))\n",
    "                       for i in range(1, len(sg_F_sum)+1)]\n",
    "df2['integrated_sg_F_sum'] = pd.Series(integrated_sg_F_sum)\n",
    "\n",
    "# sg*spike\n",
    "integrated_spike_B_sg_B_multi = [sum(abs(spike_B_sg_B_multi_sum[:i]))\n",
    "                                  for i in range(1, len(spike_B_sg_B_multi_sum)+1)]\n",
    "df2['integrated_spike_B_sg_B_multi'] = pd.Series(integrated_spike_B_sg_B_multi)\n",
    "\n",
    "integrated_spike_C_sg_D_multi = [sum(abs(spike_C_sg_D_multi_sum[:i]))\n",
    "                                 for i in range(1, len(spike_C_sg_D_multi_sum)+1)]\n",
    "df2['integrated_spike_C_sg_D_multi'] = pd.Series(integrated_spike_C_sg_D_multi)\n",
    "\n",
    "integrated_spike_D_sg_F_multi = [sum(abs(spike_D_sg_F_multi_sum[:i]))\n",
    "                                 for i in range(1, len(spike_D_sg_F_multi_sum)+1)]\n",
    "df2['integrated_spike_D_sg_F_multi'] = pd.Series(integrated_spike_D_sg_F_multi)\n",
    "############################################# New features #############################################\n",
    "\n",
    "columns = df2.columns\n",
    "transformer = StandardScaler()\n",
    "corr = pd.DataFrame(transformer.fit_transform(df2), columns=columns).corr()\n",
    "print(corr.iloc[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = df.values\n",
    "Output = Model[:, -1]\n",
    "Output = np.reshape(Output, (-1, 1))\n",
    "\n",
    "print(df2.columns)\n",
    "Model = df2.values\n",
    "Input = Model\n",
    "\n",
    "\n",
    "\n",
    "Input_shape = Input.shape[1]\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n",
    "\n",
    "Input = Input_transformer.transform(Input)\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\n",
    "total_predict = model.predict(Input)\n",
    "total_predict_actual = Output\n",
    "\n",
    "#total_predict = Output_transformer.inverse_transform(total_predict)\n",
    "#total_predict_actual = Output_transformer.inverse_transform(total_predict_actual)\n",
    "\n",
    "total_RMSE = numpy_rmse(total_predict_actual[0:25], total_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Total: ')\n",
    "print('Total RMSE:\\t', total_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predict = model.predict(Input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = '111052_projectB_ans.csv' \n",
    "\"\"\"\n",
    "load_name = '/TOPIC/projectB/projectB_template.csv'\n",
    "df = pd.read_csv(load_name)\n",
    "df3.loc[:, 'MaxWear'] = total_predict\n",
    "df3.to_csv(save_name, index=False)\n",
    "\"\"\"\n",
    "df3 = pd.DataFrame()\n",
    "index = np.arange(1, 26)\n",
    "df3['Index'] = pd.Series(index)\n",
    "df3['MaxWear'] = total_predict\n",
    "df3.to_csv(save_name, index=False)\n",
    "print('Saved to ', save_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "df60fffceb1f00e3f7f321aa5eaa19ef554ab7d6ad6d9939fc6be363c8659db3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
