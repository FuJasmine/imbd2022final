{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Flatten\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras.layers import Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'train_data/'\n",
    "\n",
    "sg_B = np.load(path + 'sg_B.npy')\n",
    "sg_C = np.load(path + 'sg_C.npy')\n",
    "sg_D = np.load(path + 'sg_D.npy')\n",
    "sg_E = np.load(path + 'sg_E.npy')\n",
    "sg_F = np.load(path + 'sg_F.npy')\n",
    "sg_G = np.load(path + 'sg_G.npy')\n",
    "sg_H = np.load(path + 'sg_H.npy')\n",
    "sg_I = np.load(path + 'sg_I.npy')\n",
    "BDF_distance = np.load(path + 'BDF_distance.npy')\n",
    "\n",
    "\n",
    "spike_A = np.load(path + 'spike_A.npy')\n",
    "spike_B = np.load(path + 'spike_B.npy')\n",
    "spike_C = np.load(path + 'spike_C.npy')\n",
    "spike_D = np.load(path + 'spike_D.npy')\n",
    "spike_abs_B = np.load(path + 'spike_abs_B.npy')\n",
    "spike_abs_C = np.load(path + 'spike_abs_C.npy')\n",
    "spike_abs_D = np.load(path + 'spike_abs_D.npy')\n",
    "\n",
    "spike_B_lower_noise = np.load(path + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise = np.load(path + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise = np.load(path + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise = np.load(path + 'spike_abs_C_lower_noise.npy')\n",
    "\n",
    "BCD_distance = np.load(path + 'BCD_distance.npy')\n",
    "BCD_abs_distance = np.load(path + 'BCD_abs_distance.npy')\n",
    "\n",
    "\n",
    "output1 = pd.read_csv('train1/00_Wear_data.csv').loc[:, 'MaxWear']\n",
    "output2 = pd.read_csv('train2/00_Wear_data.csv').loc[:, 'MaxWear']\n",
    "Output = pd.concat([output1, output2], axis=0).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_B_sum = spike_B.sum(axis=1)\n",
    "spike_C_sum = spike_C.sum(axis=1)\n",
    "spike_D_sum = spike_D.sum(axis=1)\n",
    "\n",
    "spike_B_lower_noise_sum = spike_B_lower_noise.sum(axis=1)\n",
    "spike_C_lower_noise_sum = spike_C_lower_noise.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_len = 46\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "integrated_spike_B1 = [sum(abs(spike_B_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_B2 = [sum(abs(spike_B_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_B_sum)+1)]\n",
    "integrated_spike_B = integrated_spike_B1 + integrated_spike_B2\n",
    "df2['integrated_spike_B'] = pd.Series(integrated_spike_B)\n",
    "\n",
    "\n",
    "integrated_spike_C1 = [sum(abs(spike_C_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_C2 = [sum(abs(spike_C_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_C_sum)+1)]\n",
    "integrated_spike_C = integrated_spike_C1 + integrated_spike_C2\n",
    "df2['integrated_spike_C'] = pd.Series(integrated_spike_C)\n",
    "\n",
    "integrated_spike_D1 = [sum(abs(spike_D_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_D2 = [sum(abs(spike_D_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_D_sum)+1)]\n",
    "integrated_spike_D = integrated_spike_D1 + integrated_spike_D2\n",
    "df2['integrated_spike_D'] = pd.Series(integrated_spike_D)\n",
    "\"\"\"\"\"\"\n",
    "integrated_spike_B_lower_noise1 = [sum(abs(\n",
    "    spike_B_lower_noise_sum[:i]))for i in range(1, train1_len+1)]\n",
    "integrated_spike_B_lower_noise2 = [sum(abs(\n",
    "    spike_B_lower_noise_sum[train1_len+1:i]))for i in range(train1_len+1, len(spike_B_lower_noise_sum)+1)]\n",
    "integrated_spike_B_lower_noise = integrated_spike_B_lower_noise1 + \\\n",
    "    integrated_spike_B_lower_noise2\n",
    "df2['integrated_spike_B_lower_noise'] = pd.Series(\n",
    "    integrated_spike_B_lower_noise)\n",
    "\n",
    "integrated_spike_C_lower_noise1 = [sum(abs(\n",
    "    spike_C_lower_noise_sum[:i]))for i in range(1, train1_len+1)]\n",
    "integrated_spike_C_lower_noise2 = [sum(abs(\n",
    "    spike_C_lower_noise_sum[i-1:i]))for i in range(train1_len+1, len(spike_C_lower_noise_sum)+1)]\n",
    "integrated_spike_C_lower_noise = integrated_spike_C_lower_noise1 + \\\n",
    "    integrated_spike_C_lower_noise2\n",
    "df2['integrated_spike_C_lower_noise'] = pd.Series(\n",
    "    integrated_spike_C_lower_noise)\n",
    "\n",
    "df2['Output'] = pd.Series(Output)\n",
    "\n",
    "columns = df2.columns\n",
    "transformer = StandardScaler()\n",
    "corr = pd.DataFrame(transformer.fit_transform(df2), columns=columns).corr()\n",
    "print(corr.iloc[-1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.columns)\n",
    "Model = df2.values\n",
    "Input = Model[:, :-1]\n",
    "output = Model[:, -1]\n",
    "output = np.reshape(output, (-1, 1))\n",
    "\n",
    "Input_shape = Input.shape[1]\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n",
    "print('Output layer ~ 10: ', output[:10])\n",
    "\n",
    "Input_transformer = MaxAbsScaler()\n",
    "Output_transformer = StandardScaler()\n",
    "\n",
    "Input = Input_transformer.fit_transform(Input)\n",
    "Output = Output_transformer.fit_transform(output)\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n",
    "\n",
    "input_train, input_test, output_train, output_test = train_test_split(\n",
    "    Input, output, test_size=0.1, random_state=42)\n",
    "print('input_train.shape:\\t', input_train.shape)\n",
    "print('input_test.shape:\\t', input_test.shape)\n",
    "print('output_train.shape:\\t', output_train.shape)\n",
    "print('output_test.shape:\\t', output_test.shape)\n",
    "\n",
    "input_train = tf.convert_to_tensor(input_train)\n",
    "input_test = tf.convert_to_tensor(input_test)\n",
    "output_train = tf.convert_to_tensor(output_train)\n",
    "output_test = tf.convert_to_tensor(output_test)\n",
    "\n",
    "\n",
    "# 256 B D0.2 128 B D0.2 64 B D0.2 32 B D0.2 4 1\n",
    "# 256 B D0.2 4 1\n",
    "# 256 16 4 1\n",
    "# 256 16 4 2 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=Input_shape))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "learning_rate = 0.0005    # 0.003 --> 0.0003    0.005 --> 0.0005\n",
    "batch_size = 15\n",
    "epochs = 500\n",
    "\n",
    "\n",
    "# Adam RMSprop\n",
    "model.compile(optimizer=tf.optimizers.Adam(\n",
    "    learning_rate=learning_rate), loss=rmse)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(input_train, output_train,\n",
    "                    validation_data=(input_test, output_test), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Training cost time:\\t', end - start, 's')\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_rmse(actual, predict):\n",
    "    return pow(np.mean(pow(actual - predict, 2)), 0.5)\n",
    "\n",
    "\n",
    "test_predict = model.predict(input_test)\n",
    "test_predict_actual = output_test.numpy()\n",
    "\n",
    "test_predict = np.array(test_predict)\n",
    "# print(list(test_predict))\n",
    "\n",
    "test_predict = Output_transformer.inverse_transform(test_predict)\n",
    "test_predict_actual = Output_transformer.inverse_transform(test_predict_actual)\n",
    "\n",
    "test_RMSE = numpy_rmse(test_predict_actual, test_predict)\n",
    "print('Test: ')\n",
    "print('Test RMSE:\\t', test_RMSE)\n",
    "#print('Test actual:\\t', test_predict_actual.T)\n",
    "#print('Test predict:\\t', test_predict.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predict = model.predict(input_train)\n",
    "train_predict_actual = output_train.numpy()\n",
    "\n",
    "train_predict = Output_transformer.inverse_transform(train_predict)\n",
    "train_predict_actual = Output_transformer.inverse_transform(\n",
    "    train_predict_actual)\n",
    "\n",
    "train_RMSE = numpy_rmse(train_predict_actual, train_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Train: ')\n",
    "print('Train RMSE:\\t', train_RMSE)\n",
    "#print('Train actual:\\t', train_predict_actual.T)\n",
    "#print('Train predict:\\t', train_predict.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predict = model.predict(Input)\n",
    "total_predict_actual = output\n",
    "\n",
    "total_predict = Output_transformer.inverse_transform(total_predict)\n",
    "total_predict_actual = Output_transformer.inverse_transform(\n",
    "    total_predict_actual)\n",
    "\n",
    "total_RMSE = numpy_rmse(total_predict_actual, total_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Total: ')\n",
    "print('Total RMSE:\\t', total_RMSE)\n",
    "#print('Total actual:\\t', total_predict_actual.T)\n",
    "#print('Total predict:\\t', total_predict.T)\n",
    "\n",
    "\"\"\"\n",
    "print('Total actual:')\n",
    "for i in total_predict_actual:\n",
    "        print(i[0], ', ', end='')\n",
    "print('\\nTotal predict:')\n",
    "for i in total_predict:\n",
    "        print(i[0], ', ', end='')\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(100, 30))\n",
    "plt.plot(total_predict_actual, linewidth=10.0)\n",
    "plt.plot(total_predict, color='orange', linewidth=10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'test_data/'\n",
    "\n",
    "sg_B = np.load(path + 'sg_B.npy')\n",
    "sg_C = np.load(path + 'sg_C.npy')\n",
    "sg_D = np.load(path + 'sg_D.npy')\n",
    "sg_E = np.load(path + 'sg_E.npy')\n",
    "sg_F = np.load(path + 'sg_F.npy')\n",
    "sg_G = np.load(path + 'sg_G.npy')\n",
    "sg_H = np.load(path + 'sg_H.npy')\n",
    "sg_I = np.load(path + 'sg_I.npy')\n",
    "BDF_distance = np.load(path + 'BDF_distance.npy')\n",
    "\n",
    "\n",
    "spike_A = np.load(path + 'spike_A.npy')\n",
    "spike_B = np.load(path + 'spike_B.npy')\n",
    "spike_C = np.load(path + 'spike_C.npy')\n",
    "spike_D = np.load(path + 'spike_D.npy')\n",
    "spike_abs_B = np.load(path + 'spike_abs_B.npy')\n",
    "spike_abs_C = np.load(path + 'spike_abs_C.npy')\n",
    "spike_abs_D = np.load(path + 'spike_abs_D.npy')\n",
    "\n",
    "spike_B_lower_noise = np.load(path + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise = np.load(path + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise = np.load(path + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise = np.load(path + 'spike_abs_C_lower_noise.npy')\n",
    "\n",
    "BCD_distance = np.load(path + 'BCD_distance.npy')\n",
    "BCD_abs_distance = np.load(path + 'BCD_abs_distance.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_B_sum = spike_B.sum(axis=1)\n",
    "spike_C_sum = spike_C.sum(axis=1)\n",
    "spike_D_sum = spike_D.sum(axis=1)\n",
    "\n",
    "spike_B_lower_noise_sum = spike_B_lower_noise.sum(axis=1)\n",
    "spike_C_lower_noise_sum = spike_C_lower_noise.sum(axis=1)\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "integrated_spike_B = [sum(abs(spike_B_sum[:i]))\n",
    "                      for i in range(1, len(spike_B_sum)+1)]\n",
    "df2['integrated_spike_B'] = pd.Series(integrated_spike_B)\n",
    "\n",
    "integrated_spike_C = [sum(abs(spike_C_sum[:i]))\n",
    "                      for i in range(1, len(spike_C_sum)+1)]\n",
    "df2['integrated_spike_C'] = pd.Series(integrated_spike_C)\n",
    "\n",
    "integrated_spike_D = [sum(abs(spike_D_sum[:i]))\n",
    "                      for i in range(1, len(spike_D_sum)+1)]\n",
    "df2['integrated_spike_D'] = pd.Series(integrated_spike_D)\n",
    "\"\"\"\"\"\"\n",
    "integrated_spike_B_lower_noise = [sum(abs(\n",
    "    spike_B_lower_noise_sum[:i]))for i in range(1, len(spike_B_lower_noise_sum)+1)]\n",
    "df2['integrated_spike_B_lower_noise'] = pd.Series(\n",
    "    integrated_spike_B_lower_noise)\n",
    "\n",
    "integrated_spike_C_lower_noise = [sum(abs(\n",
    "    spike_C_lower_noise_sum[:i]))for i in range(1, len(spike_C_lower_noise_sum)+1)]\n",
    "df2['integrated_spike_C_lower_noise'] = pd.Series(\n",
    "    integrated_spike_C_lower_noise)\n",
    "\n",
    "columns = df2.columns\n",
    "transformer = StandardScaler()\n",
    "corr = pd.DataFrame(transformer.fit_transform(df2), columns=columns).corr()\n",
    "print(corr.iloc[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.columns)\n",
    "Model = df2.values\n",
    "Input = Model\n",
    "\n",
    "Input_shape = Input.shape[1]\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n",
    "\n",
    "Input_transformer = MaxAbsScaler()\n",
    "\n",
    "Input = Input_transformer.fit_transform(Input)\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predict = model.predict(Input)\n",
    "total_predict = Output_transformer.inverse_transform(total_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_predict = model.predict(Input)\n",
    "total_predict_actual = output[0:25]\n",
    "\n",
    "total_predict = Output_transformer.inverse_transform(total_predict)\n",
    "total_predict_actual = Output_transformer.inverse_transform(\n",
    "    total_predict_actual)\n",
    "\n",
    "total_RMSE = numpy_rmse(total_predict_actual, total_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Total: ')\n",
    "print('Total RMSE:\\t', total_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(100, 30))\n",
    "plt.plot(total_predict_actual, linewidth=10.0)\n",
    "plt.plot(total_predict, color='orange', linewidth=10.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31b57fb55f3bd30dc4b29772f4fb5038e292b8f4ce6264100f9ef3e201d657d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
