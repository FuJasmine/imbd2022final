{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "path1 = 'train1_data/'\n",
    "path2 = 'train2_data/'\n",
    "\n",
    "sg_B1 = np.load(path1 + 'sg_B.npy')\n",
    "sg_C1 = np.load(path1 + 'sg_C.npy')\n",
    "sg_D1 = np.load(path1 + 'sg_D.npy')\n",
    "sg_E1 = np.load(path1 + 'sg_E.npy')\n",
    "sg_F1 = np.load(path1 + 'sg_F.npy')\n",
    "sg_G1 = np.load(path1 + 'sg_G.npy')\n",
    "sg_H1 = np.load(path1 + 'sg_H.npy')\n",
    "sg_I1 = np.load(path1 + 'sg_I.npy')\n",
    "BDF_distance1 = np.load(path1 + 'BDF_distance.npy')\n",
    "\n",
    "\n",
    "spike_A1 = np.load(path1 + 'spike_A.npy')\n",
    "spike_B1 = np.load(path1 + 'spike_B.npy')\n",
    "spike_C1 = np.load(path1 + 'spike_C.npy')\n",
    "spike_D1 = np.load(path1 + 'spike_D.npy')\n",
    "spike_abs_B1 = np.load(path1 + 'spike_abs_B.npy')\n",
    "spike_abs_C1 = np.load(path1 + 'spike_abs_C.npy')\n",
    "spike_abs_D1 = np.load(path1 + 'spike_abs_D.npy')\n",
    "\n",
    "spike_B_lower_noise1 = np.load(path1 + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise1 = np.load(path1 + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise1 = np.load(path1 + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise1 = np.load(path1 + 'spike_abs_C_lower_noise.npy')\n",
    "\n",
    "BCD_distance1 = np.load(path1 + 'BCD_distance.npy')\n",
    "BCD_abs_distance1 = np.load(path1 + 'BCD_abs_distance.npy')\n",
    "\n",
    "sg_B2 = np.load(path2 + 'sg_B.npy')\n",
    "sg_C2 = np.load(path2 + 'sg_C.npy')\n",
    "sg_D2 = np.load(path2 + 'sg_D.npy')\n",
    "sg_E2 = np.load(path2 + 'sg_E.npy')\n",
    "sg_F2 = np.load(path2 + 'sg_F.npy')\n",
    "sg_G2 = np.load(path2 + 'sg_G.npy')\n",
    "sg_H2 = np.load(path2 + 'sg_H.npy')\n",
    "sg_I2 = np.load(path2 + 'sg_I.npy')\n",
    "BDF_distance2 = np.load(path2 + 'BDF_distance.npy')\n",
    "\n",
    "\n",
    "spike_A2 = np.load(path2 + 'spike_A.npy')\n",
    "spike_B2 = np.load(path2 + 'spike_B.npy')\n",
    "spike_C2 = np.load(path2 + 'spike_C.npy')\n",
    "spike_D2 = np.load(path2 + 'spike_D.npy')\n",
    "spike_abs_B2 = np.load(path2 + 'spike_abs_B.npy')\n",
    "spike_abs_C2 = np.load(path2 + 'spike_abs_C.npy')\n",
    "spike_abs_D2 = np.load(path2 + 'spike_abs_D.npy')\n",
    "\n",
    "spike_B_lower_noise2 = np.load(path2 + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise2 = np.load(path2 + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise2 = np.load(path2 + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise2 = np.load(path2 + 'spike_abs_C_lower_noise.npy')\n",
    "\n",
    "BCD_distance2 = np.load(path2 + 'BCD_distance.npy')\n",
    "BCD_abs_distance2 = np.load(path2 + 'BCD_abs_distance.npy')\n",
    "\n",
    "# Concatenate the data\n",
    "sg_B = np.concatenate((sg_B1, sg_B2), axis=0)\n",
    "sg_C = np.concatenate((sg_C1, sg_C2), axis=0)\n",
    "sg_D = np.concatenate((sg_D1, sg_D2), axis=0)\n",
    "sg_E = np.concatenate((sg_E1, sg_E2), axis=0)\n",
    "sg_F = np.concatenate((sg_F1, sg_F2), axis=0)\n",
    "sg_G = np.concatenate((sg_G1, sg_G2), axis=0)\n",
    "sg_H = np.concatenate((sg_H1, sg_H2), axis=0)\n",
    "sg_I = np.concatenate((sg_I1, sg_I2), axis=0)\n",
    "BDF_distance = np.concatenate((BDF_distance1, BDF_distance2), axis=0)\n",
    "\n",
    "\n",
    "spike_A = np.concatenate((spike_A1, spike_A2), axis=0)\n",
    "spike_B = np.concatenate((spike_B1, spike_B2), axis=0)\n",
    "spike_C = np.concatenate((spike_C1, spike_C2), axis=0)\n",
    "spike_D = np.concatenate((spike_D1, spike_D2), axis=0)\n",
    "spike_abs_B = np.concatenate((spike_abs_B1, spike_abs_B2), axis=0)\n",
    "spike_abs_C = np.concatenate((spike_abs_C1, spike_abs_C2), axis=0)\n",
    "spike_abs_D = np.concatenate((spike_abs_D1, spike_abs_D2), axis=0)\n",
    "\n",
    "spike_B_lower_noise = np.concatenate((spike_B_lower_noise1, spike_B_lower_noise2), axis=0)\n",
    "spike_C_lower_noise = np.concatenate((spike_C_lower_noise1, spike_C_lower_noise2), axis=0)\n",
    "spike_abs_B_lower_noise = np.concatenate((spike_abs_B_lower_noise1, spike_abs_B_lower_noise2), axis=0)\n",
    "spike_abs_C_lower_noise = np.concatenate((spike_abs_C_lower_noise1, spike_abs_C_lower_noise2), axis=0)\n",
    "\n",
    "BCD_distance = np.concatenate((BCD_distance1, BCD_distance2), axis=0)\n",
    "BCD_abs_distance = np.concatenate((BCD_abs_distance1, BCD_abs_distance2), axis=0)\n",
    "\n",
    "output1 = pd.read_csv('train1/00_Wear_data.csv').loc[:, 'MaxWear']\n",
    "output2 = pd.read_csv('train2/00_Wear_data.csv').loc[:, 'MaxWear']\n",
    "Output = pd.concat([output1, output2], axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_B_sum = spike_B.sum(axis=1)\n",
    "spike_C_sum = spike_C.sum(axis=1)\n",
    "spike_D_sum = spike_D.sum(axis=1)\n",
    "\n",
    "spike_B_lower_noise_sum = spike_B_lower_noise.sum(axis=1)\n",
    "spike_C_lower_noise_sum = spike_C_lower_noise.sum(axis=1)\n",
    "\n",
    "spike_B_sg_B_sum = np.array([spike_B[i]-sg_B[i] for i in range(len(spike_B))]).sum(axis=1)\n",
    "spike_C_sg_D_sum = np.array([spike_C[i]-sg_D[i]\n",
    "                       for i in range(len(spike_C))]).sum(axis=1)\n",
    "spike_D_sg_F_sum = np.array([spike_D[i]-sg_F[i]\n",
    "                       for i in range(len(spike_D))]).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integrated_spike_B                0.927961\n",
      "integrated_spike_C                0.934113\n",
      "integrated_spike_D                0.938540\n",
      "integrated_spike_B_lower_noise    0.920061\n",
      "integrated_spike_C_lower_noise    0.803822\n",
      "integrated_spike_B_sg_B           0.933373\n",
      "integrated_spike_C_sg_D           0.940401\n",
      "integrated_spike_D_sg_F           0.938536\n",
      "integrated_sg_C_sum               0.879207\n",
      "Output                            1.000000\n",
      "Name: Output, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train1_len = 46\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "integrated_spike_B1 = [sum(abs(spike_B_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_B2 = [sum(abs(spike_B_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_B_sum)+1)]\n",
    "integrated_spike_B = integrated_spike_B1 + integrated_spike_B2\n",
    "df2['integrated_spike_B'] = pd.Series(integrated_spike_B)\n",
    "\n",
    "\n",
    "integrated_spike_C1 = [sum(abs(spike_C_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_C2 = [sum(abs(spike_C_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_C_sum)+1)]\n",
    "integrated_spike_C = integrated_spike_C1 + integrated_spike_C2\n",
    "df2['integrated_spike_C'] = pd.Series(integrated_spike_C)\n",
    "\n",
    "integrated_spike_D1 = [sum(abs(spike_D_sum[:i]))\n",
    "                       for i in range(1, train1_len+1)]\n",
    "integrated_spike_D2 = [sum(abs(spike_D_sum[train1_len+1:i]))\n",
    "                       for i in range(train1_len+1, len(spike_D_sum)+1)]\n",
    "integrated_spike_D = integrated_spike_D1 + integrated_spike_D2\n",
    "df2['integrated_spike_D'] = pd.Series(integrated_spike_D)\n",
    "\n",
    "integrated_spike_B_lower_noise1 = [sum(abs(\n",
    "    spike_B_lower_noise_sum[:i]))for i in range(1, train1_len+1)]\n",
    "integrated_spike_B_lower_noise2 = [sum(abs(\n",
    "    spike_B_lower_noise_sum[train1_len+1:i]))for i in range(train1_len+1, len(spike_B_lower_noise_sum)+1)]\n",
    "integrated_spike_B_lower_noise = integrated_spike_B_lower_noise1 + \\\n",
    "    integrated_spike_B_lower_noise2\n",
    "df2['integrated_spike_B_lower_noise'] = pd.Series(\n",
    "    integrated_spike_B_lower_noise)\n",
    "\n",
    "integrated_spike_C_lower_noise1 = [sum(abs(\n",
    "    spike_C_lower_noise_sum[:i]))for i in range(1, train1_len+1)]\n",
    "integrated_spike_C_lower_noise2 = [sum(abs(\n",
    "    spike_C_lower_noise_sum[i-1:i]))for i in range(train1_len+1, len(spike_C_lower_noise_sum)+1)]\n",
    "integrated_spike_C_lower_noise = integrated_spike_C_lower_noise1 + \\\n",
    "    integrated_spike_C_lower_noise2\n",
    "df2['integrated_spike_C_lower_noise'] = pd.Series(\n",
    "    integrated_spike_C_lower_noise)\n",
    "\n",
    "integrated_spike_B_sg_B1 = [sum(abs(spike_B_sg_B_sum[:i]))\n",
    "                            for i in range(1, train1_len+1)]\n",
    "integrated_spike_B_sg_B2 = [sum(abs(spike_B_sg_B_sum[train1_len+1:i]))\n",
    "                            for i in range(train1_len+1, len(spike_B_sg_B_sum)+1)]\n",
    "integrated_spike_B_sg_B = integrated_spike_B_sg_B1 + integrated_spike_B_sg_B2\n",
    "df2['integrated_spike_B_sg_B'] = pd.Series(integrated_spike_B_sg_B)\n",
    "\n",
    "integrated_spike_C_sg_D1 = [sum(abs(spike_C_sg_D_sum[:i]))\n",
    "                            for i in range(1, train1_len+1)]\n",
    "integrated_spike_C_sg_D2 = [sum(abs(spike_C_sg_D_sum[train1_len+1:i]))\n",
    "                            for i in range(train1_len+1, len(spike_C_sg_D_sum)+1)]\n",
    "integrated_spike_C_sg_D = integrated_spike_C_sg_D1 + integrated_spike_C_sg_D2\n",
    "df2['integrated_spike_C_sg_D'] = pd.Series(integrated_spike_C_sg_D)\n",
    "\n",
    "integrated_spike_D_sg_F1 = [sum(abs(spike_D_sg_F_sum[:i]))\n",
    "                            for i in range(1, train1_len+1)]\n",
    "integrated_spike_D_sg_F2 = [sum(abs(spike_D_sg_F_sum[train1_len+1:i]))\n",
    "                            for i in range(train1_len+1, len(spike_D_sg_F_sum)+1)]\n",
    "integrated_spike_D_sg_F = integrated_spike_D_sg_F1 + integrated_spike_D_sg_F2\n",
    "df2['integrated_spike_D_sg_F'] = pd.Series(integrated_spike_D_sg_F)\n",
    "\n",
    "sg_C_sum = sg_C.sum(axis=1)\n",
    "integrated_sg_C_sum1 = [sum(abs(sg_C_sum[:i]))\n",
    "                        for i in range(1, train1_len+1)]\n",
    "integrated_sg_C_sum2 = [sum(abs(sg_C_sum[train1_len+1:i]))\n",
    "                        for i in range(train1_len+1, len(sg_C_sum)+1)]\n",
    "integrated_sg_C_sum = integrated_sg_C_sum1 + integrated_sg_C_sum2\n",
    "df2['integrated_sg_C_sum'] = pd.Series(integrated_sg_C_sum)\n",
    "\n",
    "df2['Output'] = pd.Series(Output)\n",
    "\n",
    "columns = df2.columns\n",
    "transformer = StandardScaler()\n",
    "corr = pd.DataFrame(transformer.fit_transform(df2), columns=columns).corr()\n",
    "print(corr.iloc[-1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['integrated_spike_B', 'integrated_spike_C', 'integrated_spike_D',\n",
      "       'integrated_spike_B_lower_noise', 'integrated_spike_C_lower_noise',\n",
      "       'integrated_spike_B_sg_B', 'integrated_spike_C_sg_D',\n",
      "       'integrated_spike_D_sg_F', 'integrated_sg_C_sum', 'Output'],\n",
      "      dtype='object')\n",
      "Input layer 0:  [2.50166390e+01 9.57113200e+00 9.91540000e-01 6.76586250e+00\n",
      " 2.59152500e+00 2.00785639e+02 8.65901320e+01 9.91540000e-01\n",
      " 8.23949464e-02]\n",
      "Output layer ~ 10:  [[0.10236723]\n",
      " [0.13203094]\n",
      " [0.18754876]\n",
      " [0.19920984]\n",
      " [0.22729473]\n",
      " [0.26207539]\n",
      " [0.2838152 ]\n",
      " [0.30276893]\n",
      " [0.31918277]\n",
      " [0.33329432]]\n",
      "Input layer 0:  [0.0181653  0.00746002 0.01006404 0.0023528  0.00850592 0.02846079\n",
      " 0.02465728 0.01006098 0.0009187 ]\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns)\n",
    "Model = df2.values\n",
    "Input = Model[:, :-1]\n",
    "output = Model[:, -1]\n",
    "output = np.reshape(output, (-1, 1))\n",
    "\n",
    "Input_shape = Input.shape[1]\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n",
    "print('Output layer ~ 10: ', output[:10])\n",
    "\n",
    "Input_transformer = MaxAbsScaler()\n",
    "Output_transformer = StandardScaler()\n",
    "\n",
    "Input = Input_transformer.fit_transform(Input)\n",
    "Output = Output_transformer.fit_transform(output)\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_train.shape:\t (63, 9)\n",
      "input_test.shape:\t (7, 9)\n",
      "output_train.shape:\t (63, 1)\n",
      "output_test.shape:\t (7, 1)\n"
     ]
    }
   ],
   "source": [
    "input_train, input_test, output_train, output_test = train_test_split(\n",
    "    Input, output, test_size=0.1, random_state=42)\n",
    "print('input_train.shape:\\t', input_train.shape)\n",
    "print('input_test.shape:\\t', input_test.shape)\n",
    "print('output_train.shape:\\t', output_train.shape)\n",
    "print('output_test.shape:\\t', output_test.shape)\n",
    "\n",
    "input_train = tf.convert_to_tensor(input_train)\n",
    "input_test = tf.convert_to_tensor(input_test)\n",
    "output_train = tf.convert_to_tensor(output_train)\n",
    "output_test = tf.convert_to_tensor(output_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_33 (Dense)            (None, 128)               1280      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 4)                 516       \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,801\n",
      "Trainable params: 1,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=Input_shape))\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "#model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "5/5 [==============================] - 0s 29ms/step - loss: 0.2572 - val_loss: 0.1789\n",
      "Epoch 2/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1570 - val_loss: 0.1190\n",
      "Epoch 3/1000\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.1355 - val_loss: 0.0875\n",
      "Epoch 4/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.1329 - val_loss: 0.0789\n",
      "Epoch 5/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.1193 - val_loss: 0.0746\n",
      "Epoch 6/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.1004 - val_loss: 0.0758\n",
      "Epoch 7/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0813 - val_loss: 0.0720\n",
      "Epoch 8/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0733 - val_loss: 0.0563\n",
      "Epoch 9/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0651 - val_loss: 0.0511\n",
      "Epoch 10/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0602 - val_loss: 0.0476\n",
      "Epoch 11/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0548 - val_loss: 0.0395\n",
      "Epoch 12/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0492 - val_loss: 0.0332\n",
      "Epoch 13/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0451 - val_loss: 0.0317\n",
      "Epoch 14/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0408 - val_loss: 0.0323\n",
      "Epoch 15/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0377 - val_loss: 0.0333\n",
      "Epoch 16/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0345 - val_loss: 0.0340\n",
      "Epoch 17/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0321 - val_loss: 0.0345\n",
      "Epoch 18/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0305 - val_loss: 0.0345\n",
      "Epoch 19/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0283 - val_loss: 0.0348\n",
      "Epoch 20/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0273 - val_loss: 0.0343\n",
      "Epoch 21/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0246 - val_loss: 0.0337\n",
      "Epoch 22/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0240 - val_loss: 0.0331\n",
      "Epoch 23/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0236 - val_loss: 0.0322\n",
      "Epoch 24/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0241 - val_loss: 0.0319\n",
      "Epoch 25/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0250 - val_loss: 0.0312\n",
      "Epoch 26/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0215 - val_loss: 0.0320\n",
      "Epoch 27/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0219 - val_loss: 0.0302\n",
      "Epoch 28/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0209 - val_loss: 0.0301\n",
      "Epoch 29/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0208 - val_loss: 0.0300\n",
      "Epoch 30/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0218 - val_loss: 0.0304\n",
      "Epoch 31/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0213 - val_loss: 0.0311\n",
      "Epoch 32/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0210 - val_loss: 0.0290\n",
      "Epoch 33/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0196 - val_loss: 0.0286\n",
      "Epoch 34/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0197 - val_loss: 0.0281\n",
      "Epoch 35/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0200 - val_loss: 0.0288\n",
      "Epoch 36/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0207 - val_loss: 0.0271\n",
      "Epoch 37/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0193 - val_loss: 0.0273\n",
      "Epoch 38/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0211 - val_loss: 0.0266\n",
      "Epoch 39/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0189 - val_loss: 0.0275\n",
      "Epoch 40/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0186 - val_loss: 0.0262\n",
      "Epoch 41/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0187 - val_loss: 0.0258\n",
      "Epoch 42/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0178 - val_loss: 0.0257\n",
      "Epoch 43/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0267\n",
      "Epoch 44/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0192 - val_loss: 0.0278\n",
      "Epoch 45/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0184 - val_loss: 0.0254\n",
      "Epoch 46/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0256\n",
      "Epoch 47/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0179 - val_loss: 0.0252\n",
      "Epoch 48/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0199 - val_loss: 0.0244\n",
      "Epoch 49/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0175 - val_loss: 0.0242\n",
      "Epoch 50/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0177 - val_loss: 0.0258\n",
      "Epoch 51/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0187 - val_loss: 0.0238\n",
      "Epoch 52/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0187 - val_loss: 0.0263\n",
      "Epoch 53/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0188 - val_loss: 0.0243\n",
      "Epoch 54/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0188 - val_loss: 0.0217\n",
      "Epoch 55/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0159 - val_loss: 0.0217\n",
      "Epoch 56/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0160 - val_loss: 0.0207\n",
      "Epoch 57/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0158 - val_loss: 0.0204\n",
      "Epoch 58/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0156 - val_loss: 0.0199\n",
      "Epoch 59/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0152 - val_loss: 0.0194\n",
      "Epoch 60/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0150 - val_loss: 0.0189\n",
      "Epoch 61/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0146 - val_loss: 0.0184\n",
      "Epoch 62/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0145 - val_loss: 0.0180\n",
      "Epoch 63/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0150 - val_loss: 0.0189\n",
      "Epoch 64/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0174 - val_loss: 0.0203\n",
      "Epoch 65/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0147 - val_loss: 0.0170\n",
      "Epoch 66/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0151 - val_loss: 0.0187\n",
      "Epoch 67/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0153 - val_loss: 0.0172\n",
      "Epoch 68/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0145 - val_loss: 0.0208\n",
      "Epoch 69/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0175 - val_loss: 0.0174\n",
      "Epoch 70/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0140 - val_loss: 0.0148\n",
      "Epoch 71/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0146 - val_loss: 0.0155\n",
      "Epoch 72/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0139 - val_loss: 0.0147\n",
      "Epoch 73/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0134 - val_loss: 0.0151\n",
      "Epoch 74/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 0.0155\n",
      "Epoch 75/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0154\n",
      "Epoch 76/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0123 - val_loss: 0.0174\n",
      "Epoch 77/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0148\n",
      "Epoch 78/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0156\n",
      "Epoch 79/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0175\n",
      "Epoch 80/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0156\n",
      "Epoch 81/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0141 - val_loss: 0.0153\n",
      "Epoch 82/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0123 - val_loss: 0.0142\n",
      "Epoch 83/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0184\n",
      "Epoch 84/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 85/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0130\n",
      "Epoch 86/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0147\n",
      "Epoch 87/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0134 - val_loss: 0.0131\n",
      "Epoch 88/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0117 - val_loss: 0.0131\n",
      "Epoch 89/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0115 - val_loss: 0.0141\n",
      "Epoch 90/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0126 - val_loss: 0.0148\n",
      "Epoch 91/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0129\n",
      "Epoch 92/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0132 - val_loss: 0.0143\n",
      "Epoch 93/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0132 - val_loss: 0.0144\n",
      "Epoch 94/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0125 - val_loss: 0.0140\n",
      "Epoch 95/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0213\n",
      "Epoch 96/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0162\n",
      "Epoch 97/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0118 - val_loss: 0.0130\n",
      "Epoch 98/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0119\n",
      "Epoch 99/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0138 - val_loss: 0.0121\n",
      "Epoch 100/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0118 - val_loss: 0.0142\n",
      "Epoch 101/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 102/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0119\n",
      "Epoch 103/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0122 - val_loss: 0.0145\n",
      "Epoch 104/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0158\n",
      "Epoch 105/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0114\n",
      "Epoch 106/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0166\n",
      "Epoch 107/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0105 - val_loss: 0.0128\n",
      "Epoch 108/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0132 - val_loss: 0.0132\n",
      "Epoch 109/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0139\n",
      "Epoch 110/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0116 - val_loss: 0.0119\n",
      "Epoch 111/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0109 - val_loss: 0.0152\n",
      "Epoch 112/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0115\n",
      "Epoch 113/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0142\n",
      "Epoch 114/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0111\n",
      "Epoch 115/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0098 - val_loss: 0.0123\n",
      "Epoch 116/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 117/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 0.0107\n",
      "Epoch 118/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 0.0137\n",
      "Epoch 119/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0130 - val_loss: 0.0149\n",
      "Epoch 120/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0120 - val_loss: 0.0111\n",
      "Epoch 121/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0119 - val_loss: 0.0143\n",
      "Epoch 122/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0102\n",
      "Epoch 123/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0112\n",
      "Epoch 124/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0108\n",
      "Epoch 125/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0110\n",
      "Epoch 126/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0104\n",
      "Epoch 127/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0100 - val_loss: 0.0119\n",
      "Epoch 128/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0114 - val_loss: 0.0104\n",
      "Epoch 129/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 130/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0126\n",
      "Epoch 131/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0107\n",
      "Epoch 132/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0137 - val_loss: 0.0126\n",
      "Epoch 133/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0106 - val_loss: 0.0118\n",
      "Epoch 134/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0092 - val_loss: 0.0112\n",
      "Epoch 135/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0135\n",
      "Epoch 136/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0130\n",
      "Epoch 137/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0137 - val_loss: 0.0170\n",
      "Epoch 138/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0111 - val_loss: 0.0143\n",
      "Epoch 139/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 140/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0109\n",
      "Epoch 141/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0096 - val_loss: 0.0104\n",
      "Epoch 142/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0102\n",
      "Epoch 143/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0108 - val_loss: 0.0122\n",
      "Epoch 144/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 145/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 146/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0151\n",
      "Epoch 147/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0110 - val_loss: 0.0135\n",
      "Epoch 148/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0105\n",
      "Epoch 149/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0105\n",
      "Epoch 150/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0127\n",
      "Epoch 151/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0118 - val_loss: 0.0103\n",
      "Epoch 152/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0130 - val_loss: 0.0129\n",
      "Epoch 153/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0187\n",
      "Epoch 154/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0102\n",
      "Epoch 155/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0110 - val_loss: 0.0129\n",
      "Epoch 156/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0116\n",
      "Epoch 157/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0105 - val_loss: 0.0108\n",
      "Epoch 158/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0107\n",
      "Epoch 159/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 160/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0099\n",
      "Epoch 161/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0097 - val_loss: 0.0123\n",
      "Epoch 162/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0096 - val_loss: 0.0100\n",
      "Epoch 163/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0105\n",
      "Epoch 164/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0101\n",
      "Epoch 165/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 0.0100\n",
      "Epoch 166/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0101 - val_loss: 0.0119\n",
      "Epoch 167/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0099\n",
      "Epoch 168/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0111 - val_loss: 0.0121\n",
      "Epoch 169/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0098\n",
      "Epoch 170/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0095\n",
      "Epoch 171/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0093\n",
      "Epoch 172/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 173/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0103 - val_loss: 0.0121\n",
      "Epoch 174/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0090\n",
      "Epoch 175/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0103\n",
      "Epoch 176/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0091\n",
      "Epoch 177/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 178/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0106\n",
      "Epoch 179/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 0.0105\n",
      "Epoch 180/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0093\n",
      "Epoch 181/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 182/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 183/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 0.0111\n",
      "Epoch 184/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0116 - val_loss: 0.0105\n",
      "Epoch 185/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0114 - val_loss: 0.0092\n",
      "Epoch 186/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0097\n",
      "Epoch 187/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 188/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 189/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0100 - val_loss: 0.0098\n",
      "Epoch 190/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0094 - val_loss: 0.0113\n",
      "Epoch 191/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 192/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0095 - val_loss: 0.0096\n",
      "Epoch 193/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0092\n",
      "Epoch 194/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0096\n",
      "Epoch 195/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0088 - val_loss: 0.0089\n",
      "Epoch 196/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0088\n",
      "Epoch 197/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0089\n",
      "Epoch 198/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0117\n",
      "Epoch 199/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 200/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0119\n",
      "Epoch 201/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0092\n",
      "Epoch 202/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0122\n",
      "Epoch 203/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0100 - val_loss: 0.0091\n",
      "Epoch 204/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0079 - val_loss: 0.0098\n",
      "Epoch 205/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0124\n",
      "Epoch 206/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0087\n",
      "Epoch 207/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0110 - val_loss: 0.0119\n",
      "Epoch 208/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0111 - val_loss: 0.0110\n",
      "Epoch 209/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0109\n",
      "Epoch 210/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0101 - val_loss: 0.0110\n",
      "Epoch 211/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0087 - val_loss: 0.0109\n",
      "Epoch 212/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0147\n",
      "Epoch 213/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0109 - val_loss: 0.0090\n",
      "Epoch 214/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0124 - val_loss: 0.0095\n",
      "Epoch 215/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0092 - val_loss: 0.0094\n",
      "Epoch 216/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 217/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0085 - val_loss: 0.0093\n",
      "Epoch 218/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0092\n",
      "Epoch 219/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0102 - val_loss: 0.0154\n",
      "Epoch 220/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0126\n",
      "Epoch 221/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0107\n",
      "Epoch 222/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0093\n",
      "Epoch 223/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0096\n",
      "Epoch 224/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0141\n",
      "Epoch 225/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0095 - val_loss: 0.0088\n",
      "Epoch 226/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0097\n",
      "Epoch 227/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0083\n",
      "Epoch 228/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0100 - val_loss: 0.0091\n",
      "Epoch 229/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0092 - val_loss: 0.0081\n",
      "Epoch 230/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0090 - val_loss: 0.0086\n",
      "Epoch 231/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0092 - val_loss: 0.0085\n",
      "Epoch 232/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 233/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 234/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0109 - val_loss: 0.0094\n",
      "Epoch 235/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0081\n",
      "Epoch 236/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0078 - val_loss: 0.0104\n",
      "Epoch 237/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 0.0083\n",
      "Epoch 238/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0118\n",
      "Epoch 239/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0086\n",
      "Epoch 240/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0103 - val_loss: 0.0082\n",
      "Epoch 241/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0091 - val_loss: 0.0085\n",
      "Epoch 242/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0092\n",
      "Epoch 243/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 244/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 245/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0106 - val_loss: 0.0151\n",
      "Epoch 246/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0108 - val_loss: 0.0084\n",
      "Epoch 247/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0099\n",
      "Epoch 248/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0097 - val_loss: 0.0079\n",
      "Epoch 249/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0106 - val_loss: 0.0079\n",
      "Epoch 250/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0083\n",
      "Epoch 251/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0088 - val_loss: 0.0139\n",
      "Epoch 252/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0079\n",
      "Epoch 253/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0079 - val_loss: 0.0085\n",
      "Epoch 254/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0086\n",
      "Epoch 255/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0082\n",
      "Epoch 256/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 257/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0086\n",
      "Epoch 258/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0109\n",
      "Epoch 259/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0084 - val_loss: 0.0094\n",
      "Epoch 260/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0094 - val_loss: 0.0083\n",
      "Epoch 261/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0077\n",
      "Epoch 262/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0080\n",
      "Epoch 263/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0084\n",
      "Epoch 264/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0085 - val_loss: 0.0087\n",
      "Epoch 265/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0085\n",
      "Epoch 266/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 0.0093\n",
      "Epoch 267/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0085 - val_loss: 0.0091\n",
      "Epoch 268/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0084 - val_loss: 0.0094\n",
      "Epoch 269/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 270/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0126\n",
      "Epoch 271/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0081\n",
      "Epoch 272/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0099\n",
      "Epoch 273/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0083\n",
      "Epoch 274/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0124 - val_loss: 0.0072\n",
      "Epoch 275/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0073\n",
      "Epoch 276/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0105\n",
      "Epoch 277/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0077\n",
      "Epoch 278/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0125 - val_loss: 0.0091\n",
      "Epoch 279/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0135 - val_loss: 0.0111\n",
      "Epoch 280/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0097 - val_loss: 0.0083\n",
      "Epoch 281/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 282/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0115\n",
      "Epoch 283/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0121 - val_loss: 0.0126\n",
      "Epoch 284/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0120\n",
      "Epoch 285/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0076\n",
      "Epoch 286/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0089\n",
      "Epoch 287/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0084\n",
      "Epoch 288/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0172\n",
      "Epoch 289/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0140 - val_loss: 0.0081\n",
      "Epoch 290/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0118 - val_loss: 0.0076\n",
      "Epoch 291/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0082\n",
      "Epoch 292/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0097 - val_loss: 0.0086\n",
      "Epoch 293/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0143\n",
      "Epoch 294/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0088\n",
      "Epoch 295/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0102\n",
      "Epoch 296/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0087\n",
      "Epoch 297/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 298/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0100 - val_loss: 0.0092\n",
      "Epoch 299/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0089 - val_loss: 0.0075\n",
      "Epoch 300/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0075\n",
      "Epoch 301/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0073 - val_loss: 0.0093\n",
      "Epoch 302/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0107 - val_loss: 0.0139\n",
      "Epoch 303/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0122 - val_loss: 0.0093\n",
      "Epoch 304/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 305/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0088\n",
      "Epoch 306/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0076 - val_loss: 0.0090\n",
      "Epoch 307/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0088\n",
      "Epoch 308/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0095 - val_loss: 0.0103\n",
      "Epoch 309/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0113 - val_loss: 0.0102\n",
      "Epoch 310/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0099 - val_loss: 0.0079\n",
      "Epoch 311/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 0.0117\n",
      "Epoch 312/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0078\n",
      "Epoch 313/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 314/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0080\n",
      "Epoch 315/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 316/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 0.0078\n",
      "Epoch 317/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0068\n",
      "Epoch 318/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Epoch 319/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 320/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0065\n",
      "Epoch 321/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 322/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 323/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0101\n",
      "Epoch 324/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0069\n",
      "Epoch 325/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0067\n",
      "Epoch 326/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 327/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0081\n",
      "Epoch 328/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 329/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0067 - val_loss: 0.0075\n",
      "Epoch 330/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0097\n",
      "Epoch 331/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0080\n",
      "Epoch 332/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0083\n",
      "Epoch 333/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0077\n",
      "Epoch 334/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0076\n",
      "Epoch 335/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0099\n",
      "Epoch 336/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0103 - val_loss: 0.0078\n",
      "Epoch 337/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0089\n",
      "Epoch 338/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0101\n",
      "Epoch 339/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0106\n",
      "Epoch 340/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0087\n",
      "Epoch 341/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0092\n",
      "Epoch 342/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0073 - val_loss: 0.0084\n",
      "Epoch 343/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0121\n",
      "Epoch 344/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 345/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0078\n",
      "Epoch 346/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0087\n",
      "Epoch 347/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0078 - val_loss: 0.0093\n",
      "Epoch 348/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0088\n",
      "Epoch 349/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0100\n",
      "Epoch 350/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0076\n",
      "Epoch 351/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0110\n",
      "Epoch 352/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0102 - val_loss: 0.0068\n",
      "Epoch 353/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0080\n",
      "Epoch 354/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0072\n",
      "Epoch 355/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0105\n",
      "Epoch 356/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0084\n",
      "Epoch 357/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0138\n",
      "Epoch 358/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0104 - val_loss: 0.0073\n",
      "Epoch 359/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0108 - val_loss: 0.0082\n",
      "Epoch 360/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0075\n",
      "Epoch 361/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0070\n",
      "Epoch 362/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 363/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 364/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 365/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0068\n",
      "Epoch 366/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 367/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 368/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0071\n",
      "Epoch 369/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0075\n",
      "Epoch 370/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0072\n",
      "Epoch 371/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0067\n",
      "Epoch 372/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0073\n",
      "Epoch 373/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0086\n",
      "Epoch 374/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 375/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0076\n",
      "Epoch 376/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 377/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0070\n",
      "Epoch 378/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0074\n",
      "Epoch 379/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0071\n",
      "Epoch 380/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 381/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0091\n",
      "Epoch 382/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 383/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0078 - val_loss: 0.0110\n",
      "Epoch 384/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 385/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0069\n",
      "Epoch 386/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0068 - val_loss: 0.0073\n",
      "Epoch 387/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0077\n",
      "Epoch 388/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 389/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0094\n",
      "Epoch 390/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0076\n",
      "Epoch 391/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0073\n",
      "Epoch 392/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0089\n",
      "Epoch 393/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 394/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 395/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0085\n",
      "Epoch 396/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 397/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0108\n",
      "Epoch 398/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0090\n",
      "Epoch 399/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0087\n",
      "Epoch 400/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0099 - val_loss: 0.0070\n",
      "Epoch 401/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0104\n",
      "Epoch 402/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 403/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0074\n",
      "Epoch 404/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 405/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0087\n",
      "Epoch 406/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0061\n",
      "Epoch 407/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0064\n",
      "Epoch 408/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0063\n",
      "Epoch 409/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0097\n",
      "Epoch 410/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0065\n",
      "Epoch 411/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0066 - val_loss: 0.0074\n",
      "Epoch 412/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 413/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0062\n",
      "Epoch 414/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0078\n",
      "Epoch 415/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 416/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 417/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0076\n",
      "Epoch 418/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0077\n",
      "Epoch 419/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0074\n",
      "Epoch 420/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 421/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0086\n",
      "Epoch 422/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0088\n",
      "Epoch 423/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0123\n",
      "Epoch 424/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 425/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 426/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0080 - val_loss: 0.0067\n",
      "Epoch 427/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0063\n",
      "Epoch 428/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 429/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 430/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0061\n",
      "Epoch 431/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0090\n",
      "Epoch 432/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0069\n",
      "Epoch 433/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0070\n",
      "Epoch 434/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0085\n",
      "Epoch 435/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 436/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0089\n",
      "Epoch 437/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0064\n",
      "Epoch 438/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0066\n",
      "Epoch 439/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 440/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0070\n",
      "Epoch 441/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0104\n",
      "Epoch 442/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0071\n",
      "Epoch 443/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0086 - val_loss: 0.0102\n",
      "Epoch 444/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0065 - val_loss: 0.0073\n",
      "Epoch 445/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0077\n",
      "Epoch 446/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0072\n",
      "Epoch 447/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0079\n",
      "Epoch 448/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0084\n",
      "Epoch 449/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0082\n",
      "Epoch 450/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0082 - val_loss: 0.0064\n",
      "Epoch 451/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0088\n",
      "Epoch 452/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 453/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0065\n",
      "Epoch 454/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 455/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0123\n",
      "Epoch 456/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0084 - val_loss: 0.0070\n",
      "Epoch 457/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0124\n",
      "Epoch 458/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0159 - val_loss: 0.0066\n",
      "Epoch 459/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0098 - val_loss: 0.0065\n",
      "Epoch 460/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0085 - val_loss: 0.0077\n",
      "Epoch 461/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0068\n",
      "Epoch 462/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0115\n",
      "Epoch 463/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0074\n",
      "Epoch 464/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0077\n",
      "Epoch 465/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0104\n",
      "Epoch 466/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0078\n",
      "Epoch 467/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0071\n",
      "Epoch 468/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0076\n",
      "Epoch 469/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0075\n",
      "Epoch 470/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0099\n",
      "Epoch 471/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 472/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0072\n",
      "Epoch 473/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0070 - val_loss: 0.0073\n",
      "Epoch 474/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0074\n",
      "Epoch 475/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0076\n",
      "Epoch 476/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0069\n",
      "Epoch 477/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0073\n",
      "Epoch 478/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0079\n",
      "Epoch 479/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0082\n",
      "Epoch 480/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0079\n",
      "Epoch 481/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0068\n",
      "Epoch 482/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0070\n",
      "Epoch 483/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0094\n",
      "Epoch 484/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 485/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0114\n",
      "Epoch 486/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0103\n",
      "Epoch 487/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0112 - val_loss: 0.0092\n",
      "Epoch 488/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0106\n",
      "Epoch 489/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0085 - val_loss: 0.0090\n",
      "Epoch 490/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 0.0082\n",
      "Epoch 491/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0090\n",
      "Epoch 492/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0075\n",
      "Epoch 493/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0064 - val_loss: 0.0076\n",
      "Epoch 494/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0083\n",
      "Epoch 495/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0079\n",
      "Epoch 496/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0080\n",
      "Epoch 497/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0083\n",
      "Epoch 498/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0147\n",
      "Epoch 499/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0146 - val_loss: 0.0076\n",
      "Epoch 500/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0072\n",
      "Epoch 501/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0109 - val_loss: 0.0122\n",
      "Epoch 502/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0073\n",
      "Epoch 503/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0073\n",
      "Epoch 504/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0053 - val_loss: 0.0076\n",
      "Epoch 505/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0079\n",
      "Epoch 506/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0070\n",
      "Epoch 507/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0077\n",
      "Epoch 508/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0064\n",
      "Epoch 509/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 510/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0078\n",
      "Epoch 511/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0071\n",
      "Epoch 512/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0076\n",
      "Epoch 513/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0075\n",
      "Epoch 514/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0061 - val_loss: 0.0097\n",
      "Epoch 515/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0078\n",
      "Epoch 516/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0102\n",
      "Epoch 517/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0089\n",
      "Epoch 518/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0108\n",
      "Epoch 519/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0076\n",
      "Epoch 520/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0076\n",
      "Epoch 521/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0078\n",
      "Epoch 522/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0079\n",
      "Epoch 523/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0103\n",
      "Epoch 524/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0078\n",
      "Epoch 525/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0120\n",
      "Epoch 526/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0078\n",
      "Epoch 527/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0095\n",
      "Epoch 528/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0069\n",
      "Epoch 529/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0105\n",
      "Epoch 530/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0077\n",
      "Epoch 531/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 532/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0087\n",
      "Epoch 533/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0080\n",
      "Epoch 534/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0084\n",
      "Epoch 535/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0122\n",
      "Epoch 536/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0074 - val_loss: 0.0080\n",
      "Epoch 537/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0097\n",
      "Epoch 538/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 539/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0075\n",
      "Epoch 540/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0092\n",
      "Epoch 541/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0076\n",
      "Epoch 542/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0138\n",
      "Epoch 543/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0078\n",
      "Epoch 544/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0088 - val_loss: 0.0092\n",
      "Epoch 545/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0082\n",
      "Epoch 546/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 547/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0104\n",
      "Epoch 548/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 0.0081\n",
      "Epoch 549/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0105\n",
      "Epoch 550/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0089\n",
      "Epoch 551/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0114\n",
      "Epoch 552/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0088\n",
      "Epoch 553/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0085\n",
      "Epoch 554/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0078\n",
      "Epoch 555/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0095\n",
      "Epoch 556/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0078\n",
      "Epoch 557/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0120\n",
      "Epoch 558/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0077\n",
      "Epoch 559/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0098\n",
      "Epoch 560/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0082\n",
      "Epoch 561/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0096\n",
      "Epoch 562/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 563/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0084\n",
      "Epoch 564/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0088\n",
      "Epoch 565/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0092\n",
      "Epoch 566/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0102\n",
      "Epoch 567/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0081\n",
      "Epoch 568/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0101\n",
      "Epoch 569/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 570/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0080\n",
      "Epoch 571/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0084\n",
      "Epoch 572/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0082\n",
      "Epoch 573/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0080\n",
      "Epoch 574/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0074\n",
      "Epoch 575/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0083\n",
      "Epoch 576/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0063 - val_loss: 0.0107\n",
      "Epoch 577/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0078\n",
      "Epoch 578/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0109\n",
      "Epoch 579/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0098\n",
      "Epoch 580/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0102 - val_loss: 0.0095\n",
      "Epoch 581/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0081\n",
      "Epoch 582/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0099\n",
      "Epoch 583/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0080\n",
      "Epoch 584/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 585/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0101\n",
      "Epoch 586/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0090\n",
      "Epoch 587/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0082\n",
      "Epoch 588/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0101\n",
      "Epoch 589/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0084\n",
      "Epoch 590/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0089\n",
      "Epoch 591/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0052 - val_loss: 0.0087\n",
      "Epoch 592/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0135\n",
      "Epoch 593/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 594/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0120\n",
      "Epoch 595/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0077 - val_loss: 0.0078\n",
      "Epoch 596/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0077 - val_loss: 0.0101\n",
      "Epoch 597/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0079\n",
      "Epoch 598/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0061 - val_loss: 0.0101\n",
      "Epoch 599/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0088\n",
      "Epoch 600/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0091\n",
      "Epoch 601/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0101\n",
      "Epoch 602/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0083\n",
      "Epoch 603/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 604/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0104\n",
      "Epoch 605/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0086\n",
      "Epoch 606/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0076\n",
      "Epoch 607/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0080\n",
      "Epoch 608/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0098\n",
      "Epoch 609/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0080\n",
      "Epoch 610/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0100\n",
      "Epoch 611/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0087\n",
      "Epoch 612/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0084\n",
      "Epoch 613/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0115\n",
      "Epoch 614/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0079 - val_loss: 0.0083\n",
      "Epoch 615/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0082\n",
      "Epoch 616/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 617/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0113\n",
      "Epoch 618/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0076\n",
      "Epoch 619/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0077\n",
      "Epoch 620/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0075\n",
      "Epoch 621/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0077\n",
      "Epoch 622/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0075\n",
      "Epoch 623/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0081\n",
      "Epoch 624/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0104\n",
      "Epoch 625/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0076\n",
      "Epoch 626/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 627/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0075\n",
      "Epoch 628/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0091\n",
      "Epoch 629/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0074\n",
      "Epoch 630/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0099\n",
      "Epoch 631/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0084\n",
      "Epoch 632/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0086\n",
      "Epoch 633/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0096\n",
      "Epoch 634/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0077\n",
      "Epoch 635/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0097\n",
      "Epoch 636/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0082\n",
      "Epoch 637/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0110\n",
      "Epoch 638/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0080\n",
      "Epoch 639/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0079\n",
      "Epoch 640/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0087\n",
      "Epoch 641/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0085\n",
      "Epoch 642/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0080\n",
      "Epoch 643/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0092\n",
      "Epoch 644/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0089\n",
      "Epoch 645/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 646/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0112\n",
      "Epoch 647/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0129 - val_loss: 0.0076\n",
      "Epoch 648/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0084\n",
      "Epoch 649/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0085\n",
      "Epoch 650/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0110\n",
      "Epoch 651/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0086\n",
      "Epoch 652/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0079 - val_loss: 0.0087\n",
      "Epoch 653/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0095\n",
      "Epoch 654/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0125\n",
      "Epoch 655/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0092 - val_loss: 0.0082\n",
      "Epoch 656/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 657/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0063 - val_loss: 0.0075\n",
      "Epoch 658/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0085\n",
      "Epoch 659/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0083\n",
      "Epoch 660/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0090\n",
      "Epoch 661/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0095\n",
      "Epoch 662/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0078 - val_loss: 0.0085\n",
      "Epoch 663/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0093\n",
      "Epoch 664/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0076\n",
      "Epoch 665/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0120\n",
      "Epoch 666/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0084\n",
      "Epoch 667/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0082 - val_loss: 0.0106\n",
      "Epoch 668/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0128 - val_loss: 0.0080\n",
      "Epoch 669/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0085\n",
      "Epoch 670/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0147\n",
      "Epoch 671/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0097\n",
      "Epoch 672/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0077 - val_loss: 0.0107\n",
      "Epoch 673/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0134 - val_loss: 0.0102\n",
      "Epoch 674/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0112 - val_loss: 0.0085\n",
      "Epoch 675/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0093 - val_loss: 0.0105\n",
      "Epoch 676/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0086\n",
      "Epoch 677/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0078\n",
      "Epoch 678/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0069 - val_loss: 0.0084\n",
      "Epoch 679/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0078\n",
      "Epoch 680/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0098\n",
      "Epoch 681/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 682/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0061 - val_loss: 0.0110\n",
      "Epoch 683/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0082\n",
      "Epoch 684/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0085\n",
      "Epoch 685/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0110\n",
      "Epoch 686/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0083\n",
      "Epoch 687/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 688/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0098\n",
      "Epoch 689/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0080\n",
      "Epoch 690/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0100\n",
      "Epoch 691/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0070\n",
      "Epoch 692/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0069\n",
      "Epoch 693/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0058 - val_loss: 0.0097\n",
      "Epoch 694/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0080\n",
      "Epoch 695/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0088\n",
      "Epoch 696/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0072\n",
      "Epoch 697/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0105\n",
      "Epoch 698/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0078\n",
      "Epoch 699/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0116 - val_loss: 0.0087\n",
      "Epoch 700/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0091\n",
      "Epoch 701/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0082\n",
      "Epoch 702/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0081\n",
      "Epoch 703/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0083\n",
      "Epoch 704/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0094\n",
      "Epoch 705/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0086\n",
      "Epoch 706/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0086\n",
      "Epoch 707/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0088\n",
      "Epoch 708/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0080\n",
      "Epoch 709/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0085\n",
      "Epoch 710/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0073\n",
      "Epoch 711/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0076\n",
      "Epoch 712/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 713/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0079\n",
      "Epoch 714/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0053 - val_loss: 0.0080\n",
      "Epoch 715/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0056 - val_loss: 0.0105\n",
      "Epoch 716/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0060 - val_loss: 0.0076\n",
      "Epoch 717/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0080\n",
      "Epoch 718/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0079\n",
      "Epoch 719/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0089\n",
      "Epoch 720/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0085\n",
      "Epoch 721/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0104\n",
      "Epoch 722/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0080\n",
      "Epoch 723/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0080\n",
      "Epoch 724/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0090\n",
      "Epoch 725/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0092\n",
      "Epoch 726/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0083\n",
      "Epoch 727/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0088\n",
      "Epoch 728/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0048 - val_loss: 0.0081\n",
      "Epoch 729/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0100\n",
      "Epoch 730/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0084\n",
      "Epoch 731/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0078\n",
      "Epoch 732/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0084\n",
      "Epoch 733/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0045 - val_loss: 0.0092\n",
      "Epoch 734/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0083\n",
      "Epoch 735/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0048 - val_loss: 0.0114\n",
      "Epoch 736/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0082\n",
      "Epoch 737/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0095\n",
      "Epoch 738/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0096\n",
      "Epoch 739/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0089\n",
      "Epoch 740/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0090\n",
      "Epoch 741/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0090\n",
      "Epoch 742/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0086\n",
      "Epoch 743/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0100\n",
      "Epoch 744/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0090\n",
      "Epoch 745/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0092\n",
      "Epoch 746/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0083\n",
      "Epoch 747/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0092\n",
      "Epoch 748/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0099\n",
      "Epoch 749/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0083\n",
      "Epoch 750/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0086\n",
      "Epoch 751/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0103\n",
      "Epoch 752/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0095\n",
      "Epoch 753/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0142\n",
      "Epoch 754/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0136 - val_loss: 0.0091\n",
      "Epoch 755/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0104 - val_loss: 0.0091\n",
      "Epoch 756/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0068 - val_loss: 0.0084\n",
      "Epoch 757/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0097\n",
      "Epoch 758/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0080\n",
      "Epoch 759/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0114\n",
      "Epoch 760/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0105 - val_loss: 0.0085\n",
      "Epoch 761/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0111 - val_loss: 0.0122\n",
      "Epoch 762/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0141 - val_loss: 0.0141\n",
      "Epoch 763/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 764/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0125\n",
      "Epoch 765/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0069 - val_loss: 0.0083\n",
      "Epoch 766/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0082 - val_loss: 0.0115\n",
      "Epoch 767/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0077\n",
      "Epoch 768/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0106\n",
      "Epoch 769/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0076\n",
      "Epoch 770/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0095\n",
      "Epoch 771/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 772/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0107\n",
      "Epoch 773/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 774/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0085\n",
      "Epoch 775/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0098\n",
      "Epoch 776/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0099\n",
      "Epoch 777/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0087\n",
      "Epoch 778/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0138\n",
      "Epoch 779/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0083\n",
      "Epoch 780/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0050 - val_loss: 0.0079\n",
      "Epoch 781/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0104\n",
      "Epoch 782/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0082\n",
      "Epoch 783/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0100\n",
      "Epoch 784/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0086\n",
      "Epoch 785/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0115\n",
      "Epoch 786/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 787/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0086 - val_loss: 0.0087\n",
      "Epoch 788/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0083\n",
      "Epoch 789/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0095\n",
      "Epoch 790/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0094\n",
      "Epoch 791/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0078\n",
      "Epoch 792/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0115\n",
      "Epoch 793/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0087 - val_loss: 0.0077\n",
      "Epoch 794/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0097\n",
      "Epoch 795/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0081\n",
      "Epoch 796/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0102\n",
      "Epoch 797/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0079\n",
      "Epoch 798/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0089\n",
      "Epoch 799/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0125\n",
      "Epoch 800/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0096 - val_loss: 0.0082\n",
      "Epoch 801/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0075 - val_loss: 0.0081\n",
      "Epoch 802/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0110\n",
      "Epoch 803/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0101\n",
      "Epoch 804/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0102\n",
      "Epoch 805/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0094\n",
      "Epoch 806/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0074 - val_loss: 0.0095\n",
      "Epoch 807/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0093\n",
      "Epoch 808/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0099\n",
      "Epoch 809/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0082\n",
      "Epoch 810/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0111\n",
      "Epoch 811/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0076\n",
      "Epoch 812/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0080\n",
      "Epoch 813/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 814/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0096\n",
      "Epoch 815/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0105\n",
      "Epoch 816/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0070 - val_loss: 0.0083\n",
      "Epoch 817/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0110\n",
      "Epoch 818/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0082\n",
      "Epoch 819/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0083\n",
      "Epoch 820/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0099\n",
      "Epoch 821/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0105\n",
      "Epoch 822/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0083\n",
      "Epoch 823/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0070 - val_loss: 0.0092\n",
      "Epoch 824/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0078\n",
      "Epoch 825/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0058 - val_loss: 0.0106\n",
      "Epoch 826/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0059 - val_loss: 0.0102\n",
      "Epoch 827/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0100\n",
      "Epoch 828/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0084\n",
      "Epoch 829/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0082\n",
      "Epoch 830/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0092\n",
      "Epoch 831/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 832/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0087 - val_loss: 0.0098\n",
      "Epoch 833/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0077 - val_loss: 0.0079\n",
      "Epoch 834/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 835/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0085\n",
      "Epoch 836/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0104\n",
      "Epoch 837/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0059 - val_loss: 0.0082\n",
      "Epoch 838/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0084\n",
      "Epoch 839/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0083\n",
      "Epoch 840/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 841/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0075\n",
      "Epoch 842/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0057 - val_loss: 0.0100\n",
      "Epoch 843/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0078\n",
      "Epoch 844/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0064 - val_loss: 0.0116\n",
      "Epoch 845/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0093 - val_loss: 0.0082\n",
      "Epoch 846/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0047 - val_loss: 0.0087\n",
      "Epoch 847/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0085\n",
      "Epoch 848/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0086\n",
      "Epoch 849/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0087\n",
      "Epoch 850/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0083\n",
      "Epoch 851/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0079\n",
      "Epoch 852/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0076\n",
      "Epoch 853/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0095\n",
      "Epoch 854/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0078\n",
      "Epoch 855/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0060 - val_loss: 0.0104\n",
      "Epoch 856/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0085\n",
      "Epoch 857/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0100\n",
      "Epoch 858/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0076\n",
      "Epoch 859/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0113\n",
      "Epoch 860/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0073\n",
      "Epoch 861/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0105\n",
      "Epoch 862/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0084\n",
      "Epoch 863/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0090 - val_loss: 0.0094\n",
      "Epoch 864/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0072 - val_loss: 0.0090\n",
      "Epoch 865/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0098 - val_loss: 0.0136\n",
      "Epoch 866/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0071 - val_loss: 0.0077\n",
      "Epoch 867/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 868/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0094\n",
      "Epoch 869/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0064 - val_loss: 0.0093\n",
      "Epoch 870/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0095\n",
      "Epoch 871/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0074 - val_loss: 0.0105\n",
      "Epoch 872/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0062 - val_loss: 0.0117\n",
      "Epoch 873/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0067 - val_loss: 0.0088\n",
      "Epoch 874/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0086\n",
      "Epoch 875/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 876/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0051 - val_loss: 0.0078\n",
      "Epoch 877/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0044 - val_loss: 0.0100\n",
      "Epoch 878/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0079\n",
      "Epoch 879/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0114\n",
      "Epoch 880/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0066 - val_loss: 0.0090\n",
      "Epoch 881/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0087\n",
      "Epoch 882/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0061 - val_loss: 0.0090\n",
      "Epoch 883/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0066 - val_loss: 0.0089\n",
      "Epoch 884/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0083\n",
      "Epoch 885/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0084\n",
      "Epoch 886/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0076\n",
      "Epoch 887/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0077\n",
      "Epoch 888/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0093\n",
      "Epoch 889/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0084\n",
      "Epoch 890/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0086\n",
      "Epoch 891/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0050 - val_loss: 0.0096\n",
      "Epoch 892/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0082\n",
      "Epoch 893/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0061 - val_loss: 0.0117\n",
      "Epoch 894/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0078 - val_loss: 0.0091\n",
      "Epoch 895/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0068 - val_loss: 0.0102\n",
      "Epoch 896/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0081\n",
      "Epoch 897/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0084\n",
      "Epoch 898/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0095\n",
      "Epoch 899/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0085\n",
      "Epoch 900/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0062 - val_loss: 0.0090\n",
      "Epoch 901/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0073\n",
      "Epoch 902/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 903/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0091\n",
      "Epoch 904/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0085\n",
      "Epoch 905/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0082\n",
      "Epoch 906/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0091\n",
      "Epoch 907/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0071 - val_loss: 0.0099\n",
      "Epoch 908/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0079\n",
      "Epoch 909/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0073 - val_loss: 0.0080\n",
      "Epoch 910/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 911/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0061 - val_loss: 0.0110\n",
      "Epoch 912/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0066 - val_loss: 0.0097\n",
      "Epoch 913/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0067 - val_loss: 0.0085\n",
      "Epoch 914/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0051 - val_loss: 0.0078\n",
      "Epoch 915/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0049 - val_loss: 0.0085\n",
      "Epoch 916/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0084\n",
      "Epoch 917/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0055 - val_loss: 0.0088\n",
      "Epoch 918/1000\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.0052 - val_loss: 0.0097\n",
      "Epoch 919/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0091\n",
      "Epoch 920/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0117\n",
      "Epoch 921/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0100 - val_loss: 0.0092\n",
      "Epoch 922/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0092 - val_loss: 0.0102\n",
      "Epoch 923/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0080 - val_loss: 0.0081\n",
      "Epoch 924/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0086 - val_loss: 0.0077\n",
      "Epoch 925/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0082\n",
      "Epoch 926/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0080\n",
      "Epoch 927/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0054 - val_loss: 0.0093\n",
      "Epoch 928/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0091\n",
      "Epoch 929/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0140\n",
      "Epoch 930/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0088 - val_loss: 0.0090\n",
      "Epoch 931/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0089 - val_loss: 0.0123\n",
      "Epoch 932/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0117 - val_loss: 0.0099\n",
      "Epoch 933/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0076 - val_loss: 0.0093\n",
      "Epoch 934/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0101\n",
      "Epoch 935/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0087\n",
      "Epoch 936/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0085\n",
      "Epoch 937/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0046 - val_loss: 0.0088\n",
      "Epoch 938/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0111\n",
      "Epoch 939/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0079\n",
      "Epoch 940/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0089\n",
      "Epoch 941/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0075 - val_loss: 0.0089\n",
      "Epoch 942/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0065 - val_loss: 0.0101\n",
      "Epoch 943/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0072 - val_loss: 0.0092\n",
      "Epoch 944/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0078\n",
      "Epoch 945/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0078\n",
      "Epoch 946/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0089\n",
      "Epoch 947/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0100\n",
      "Epoch 948/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0087\n",
      "Epoch 949/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0062 - val_loss: 0.0085\n",
      "Epoch 950/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0088\n",
      "Epoch 951/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0069 - val_loss: 0.0108\n",
      "Epoch 952/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0077\n",
      "Epoch 953/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0079 - val_loss: 0.0098\n",
      "Epoch 954/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0095 - val_loss: 0.0085\n",
      "Epoch 955/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0055 - val_loss: 0.0100\n",
      "Epoch 956/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0088\n",
      "Epoch 957/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0096\n",
      "Epoch 958/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0083\n",
      "Epoch 959/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0056 - val_loss: 0.0112\n",
      "Epoch 960/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0065 - val_loss: 0.0087\n",
      "Epoch 961/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0093\n",
      "Epoch 962/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0083\n",
      "Epoch 963/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0091\n",
      "Epoch 964/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0052 - val_loss: 0.0101\n",
      "Epoch 965/1000\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.0048 - val_loss: 0.0088\n",
      "Epoch 966/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0053 - val_loss: 0.0087\n",
      "Epoch 967/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0090\n",
      "Epoch 968/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0088\n",
      "Epoch 969/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0102\n",
      "Epoch 970/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0082\n",
      "Epoch 971/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0047 - val_loss: 0.0095\n",
      "Epoch 972/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0050 - val_loss: 0.0082\n",
      "Epoch 973/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0111\n",
      "Epoch 974/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0054 - val_loss: 0.0094\n",
      "Epoch 975/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0058 - val_loss: 0.0115\n",
      "Epoch 976/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0083 - val_loss: 0.0102\n",
      "Epoch 977/1000\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.0128 - val_loss: 0.0092\n",
      "Epoch 978/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 979/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 980/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0057 - val_loss: 0.0094\n",
      "Epoch 981/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0049 - val_loss: 0.0083\n",
      "Epoch 982/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0084\n",
      "Epoch 983/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0046 - val_loss: 0.0094\n",
      "Epoch 984/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0055 - val_loss: 0.0088\n",
      "Epoch 985/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0056 - val_loss: 0.0080\n",
      "Epoch 986/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0059 - val_loss: 0.0092\n",
      "Epoch 987/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0048 - val_loss: 0.0087\n",
      "Epoch 988/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0057 - val_loss: 0.0084\n",
      "Epoch 989/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0060 - val_loss: 0.0088\n",
      "Epoch 990/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0052 - val_loss: 0.0099\n",
      "Epoch 991/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0081\n",
      "Epoch 992/1000\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.0052 - val_loss: 0.0081\n",
      "Epoch 993/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0043 - val_loss: 0.0079\n",
      "Epoch 994/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0045 - val_loss: 0.0091\n",
      "Epoch 995/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0046 - val_loss: 0.0082\n",
      "Epoch 996/1000\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.0047 - val_loss: 0.0079\n",
      "Epoch 997/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0049 - val_loss: 0.0079\n",
      "Epoch 998/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0054 - val_loss: 0.0111\n",
      "Epoch 999/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0063 - val_loss: 0.0082\n",
      "Epoch 1000/1000\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.0053 - val_loss: 0.0085\n",
      "\n",
      "\n",
      "\n",
      "Training cost time:\t 41.92203187942505 s\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 15\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "# Adam RMSprop\n",
    "model.compile(optimizer=tf.optimizers.Adam(\n",
    "    learning_rate=learning_rate), loss=rmse)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(input_train, output_train,\n",
    "                    validation_data=(input_test, output_test), \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs,\n",
    "                    verbose=1)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n\\n')\n",
    "print('Training cost time:\\t', end - start, 's')\n",
    "print('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "Test: \n",
      "Test RMSE:\t 0.00128065527779953\n"
     ]
    }
   ],
   "source": [
    "def numpy_rmse(actual, predict):\n",
    "    return pow(np.mean(pow(actual - predict, 2)), 0.5)\n",
    "\n",
    "\n",
    "test_predict = model.predict(input_test)\n",
    "test_predict_actual = output_test.numpy()\n",
    "\n",
    "test_predict = np.array(test_predict)\n",
    "\n",
    "test_predict = Output_transformer.inverse_transform(test_predict)\n",
    "test_predict_actual = Output_transformer.inverse_transform(test_predict_actual)\n",
    "\n",
    "test_RMSE = numpy_rmse(test_predict_actual, test_predict)\n",
    "print('Test: ')\n",
    "print('Test RMSE:\\t', test_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Train: \n",
      "Train RMSE:\t 0.0008463858412094719\n"
     ]
    }
   ],
   "source": [
    "train_predict = model.predict(input_train)\n",
    "train_predict_actual = output_train.numpy()\n",
    "\n",
    "train_predict = Output_transformer.inverse_transform(train_predict)\n",
    "train_predict_actual = Output_transformer.inverse_transform(\n",
    "    train_predict_actual)\n",
    "\n",
    "train_RMSE = numpy_rmse(train_predict_actual, train_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Train: ')\n",
    "print('Train RMSE:\\t', train_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total: \n",
      "Total RMSE:\t 0.0008992996647588932\n"
     ]
    }
   ],
   "source": [
    "total_predict = model.predict(Input)\n",
    "total_predict_actual = output\n",
    "\n",
    "total_predict = Output_transformer.inverse_transform(total_predict)\n",
    "total_predict_actual = Output_transformer.inverse_transform(\n",
    "    total_predict_actual)\n",
    "\n",
    "total_RMSE = numpy_rmse(total_predict_actual, total_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Total: ')\n",
    "print('Total RMSE:\\t', total_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "path3 = 'test_data/'\n",
    "\n",
    "sg_B = np.load(path3 + 'sg_B.npy')\n",
    "sg_C = np.load(path3 + 'sg_C.npy')\n",
    "sg_D = np.load(path3 + 'sg_D.npy')\n",
    "sg_E = np.load(path3 + 'sg_E.npy')\n",
    "sg_F = np.load(path3 + 'sg_F.npy')\n",
    "sg_G = np.load(path3 + 'sg_G.npy')\n",
    "sg_H = np.load(path3 + 'sg_H.npy')\n",
    "sg_I = np.load(path3 + 'sg_I.npy')\n",
    "BDF_distance = np.load(path3 + 'BDF_distance.npy')\n",
    "\n",
    "\n",
    "spike_A = np.load(path3 + 'spike_A.npy')\n",
    "spike_B = np.load(path3 + 'spike_B.npy')\n",
    "spike_C = np.load(path3 + 'spike_C.npy')\n",
    "spike_D = np.load(path3 + 'spike_D.npy')\n",
    "spike_abs_B = np.load(path3 + 'spike_abs_B.npy')\n",
    "spike_abs_C = np.load(path3 + 'spike_abs_C.npy')\n",
    "spike_abs_D = np.load(path3 + 'spike_abs_D.npy')\n",
    "\n",
    "spike_B_lower_noise = np.load(path3 + 'spike_B_lower_noise.npy')\n",
    "spike_C_lower_noise = np.load(path3 + 'spike_C_lower_noise.npy')\n",
    "spike_abs_B_lower_noise = np.load(path3 + 'spike_abs_B_lower_noise.npy')\n",
    "spike_abs_C_lower_noise = np.load(path3 + 'spike_abs_C_lower_noise.npy')\n",
    "\n",
    "BCD_distance = np.load(path3 + 'BCD_distance.npy')\n",
    "BCD_abs_distance = np.load(path3 + 'BCD_abs_distance.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test data...\n",
      "integrated_spike_B                0.996059\n",
      "integrated_spike_C                0.993350\n",
      "integrated_spike_D                0.987968\n",
      "integrated_spike_B_lower_noise    0.994362\n",
      "integrated_spike_C_lower_noise    0.991908\n",
      "integrated_spike_B_sg_B_sum       0.995036\n",
      "integrated_spike_C_sg_D_sum       0.988395\n",
      "integrated_spike_D_sg_F_sum       0.987970\n",
      "integrated_sg_C_sum               1.000000\n",
      "Name: integrated_sg_C_sum, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('preprocessing test data...')\n",
    "spike_B_sum = spike_B.sum(axis=1)\n",
    "spike_C_sum = spike_C.sum(axis=1)\n",
    "spike_D_sum = spike_D.sum(axis=1)\n",
    "\n",
    "spike_B_lower_noise_sum = spike_B_lower_noise.sum(axis=1)\n",
    "spike_C_lower_noise_sum = spike_C_lower_noise.sum(axis=1)\n",
    "\n",
    "spike_B_sg_B_sum = np.array([spike_B[i]-sg_B[i]\n",
    "                            for i in range(len(spike_B))]).sum(axis=1)\n",
    "spike_C_sg_D_sum = np.array([spike_C[i]-sg_D[i]\n",
    "                             for i in range(len(spike_C))]).sum(axis=1)\n",
    "spike_D_sg_F_sum = np.array([spike_D[i]-sg_F[i]\n",
    "                             for i in range(len(spike_D))]).sum(axis=1)\n",
    "\n",
    "df2 = pd.DataFrame()\n",
    "integrated_spike_B = [sum(abs(spike_B_sum[:i]))\n",
    "                      for i in range(1, len(spike_B_sum)+1)]\n",
    "df2['integrated_spike_B'] = pd.Series(integrated_spike_B)\n",
    "\n",
    "integrated_spike_C = [sum(abs(spike_C_sum[:i]))\n",
    "                      for i in range(1, len(spike_C_sum)+1)]\n",
    "df2['integrated_spike_C'] = pd.Series(integrated_spike_C)\n",
    "\n",
    "integrated_spike_D = [sum(abs(spike_D_sum[:i]))\n",
    "                      for i in range(1, len(spike_D_sum)+1)]\n",
    "df2['integrated_spike_D'] = pd.Series(integrated_spike_D)\n",
    "\n",
    "integrated_spike_B_lower_noise = [sum(abs(\n",
    "    spike_B_lower_noise_sum[:i]))for i in range(1, len(spike_B_lower_noise_sum)+1)]\n",
    "df2['integrated_spike_B_lower_noise'] = pd.Series(\n",
    "    integrated_spike_B_lower_noise)\n",
    "\n",
    "integrated_spike_C_lower_noise = [sum(abs(\n",
    "    spike_C_lower_noise_sum[:i]))for i in range(1, len(spike_C_lower_noise_sum)+1)]\n",
    "df2['integrated_spike_C_lower_noise'] = pd.Series(\n",
    "    integrated_spike_C_lower_noise)\n",
    "\n",
    "integrated_spike_B_sg_B_sum = [sum(abs(spike_B_sg_B_sum[:i]))\n",
    "                      for i in range(1, len(spike_B_sg_B_sum)+1)]\n",
    "df2['integrated_spike_B_sg_B_sum'] = pd.Series(integrated_spike_B_sg_B_sum)\n",
    "\n",
    "integrated_spike_C_sg_D_sum = [sum(abs(spike_C_sg_D_sum[:i]))\n",
    "                               for i in range(1, len(spike_C_sg_D_sum)+1)]\n",
    "df2['integrated_spike_C_sg_D_sum'] = pd.Series(integrated_spike_C_sg_D_sum)\n",
    "\n",
    "integrated_spike_D_sg_F_sum = [sum(abs(spike_D_sg_F_sum[:i]))\n",
    "                               for i in range(1, len(spike_D_sg_F_sum)+1)]\n",
    "df2['integrated_spike_D_sg_F_sum'] = pd.Series(integrated_spike_D_sg_F_sum)\n",
    "\n",
    "integrated_sg_C_sum = [sum(abs(sg_C_sum[:i]))\n",
    "                       for i in range(1, len(sg_C_sum)+1)]\n",
    "df2['integrated_sg_C_sum'] = pd.Series(integrated_sg_C_sum)\n",
    "\n",
    "columns = df2.columns\n",
    "transformer = StandardScaler()\n",
    "corr = pd.DataFrame(transformer.fit_transform(df2), columns=columns).corr()\n",
    "print(corr.iloc[-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['integrated_spike_B', 'integrated_spike_C', 'integrated_spike_D',\n",
      "       'integrated_spike_B_lower_noise', 'integrated_spike_C_lower_noise',\n",
      "       'integrated_spike_B_sg_B_sum', 'integrated_spike_C_sg_D_sum',\n",
      "       'integrated_spike_D_sg_F_sum', 'integrated_sg_C_sum'],\n",
      "      dtype='object')\n",
      "Input layer 0:  [2.50166390e+01 9.57113200e+00 9.91540000e-01 6.76586250e+00\n",
      " 2.59152500e+00 2.00785639e+02 8.65901320e+01 9.91540000e-01\n",
      " 8.23949464e-02]\n",
      "Input layer 0:  [0.0181653  0.00746002 0.01006404 0.0023528  0.00850592 0.02846079\n",
      " 0.02465728 0.01006098 0.0009187 ]\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns)\n",
    "Model = df2.values\n",
    "Input = Model\n",
    "\n",
    "Input_shape = Input.shape[1]\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n",
    "\n",
    "Input = Input_transformer.transform(Input)\n",
    "\n",
    "print('Input layer 0: ', Input[0, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Total: \n",
      "Total RMSE:\t 0.0010354539514033378\n"
     ]
    }
   ],
   "source": [
    "total_predict = model.predict(Input)\n",
    "total_predict_actual = output\n",
    "\n",
    "total_predict = Output_transformer.inverse_transform(total_predict)\n",
    "total_predict_actual = Output_transformer.inverse_transform(\n",
    "    total_predict_actual)\n",
    "\n",
    "total_RMSE = numpy_rmse(total_predict_actual[0:25], total_predict)\n",
    "print('\\n\\n\\n')\n",
    "print('Total: ')\n",
    "print('Total RMSE:\\t', total_RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "total_predict = model.predict(Input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to  111052_projectB_ans.csv\n"
     ]
    }
   ],
   "source": [
    "save_name = '111052_projectB_ans.csv'  \n",
    "\n",
    "# load_name = '111052_projectB_ans.csv'\n",
    "# df3 = pd.read_csv(load_name)\n",
    "# df3.loc[:, 'MaxWear'] = total_predict\n",
    "# df3.to_csv(save_name, index=False)\n",
    "\n",
    "df3 = pd.DataFrame()\n",
    "index = np.arange(1, 26)\n",
    "df3['Index'] = pd.Series(index)\n",
    "df3['MaxWear'] = total_predict\n",
    "df3.to_csv(save_name, index=False)\n",
    "print('Saved to ', save_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31b57fb55f3bd30dc4b29772f4fb5038e292b8f4ce6264100f9ef3e201d657d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
